{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import pandas as pd\n",
    "import re,nltk, string, unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.options.display.max_rows = 500\n",
    "df = pd.read_csv(\"Data/formattedData.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ricardo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dfSntTok = df.dropna()\n",
    "dfSntTok[\"Respuesta\"] = dfSntTok[\"Respuesta\"].apply(nltk.sent_tokenize)\n",
    "dfSntTok = dfSntTok[dfSntTok['Respuesta'].apply(len) >= 1].dropna()\n",
    "\n",
    "arrayExterno = []\n",
    "def separeSentences(row):\n",
    "    if type(row['Respuesta']) != str:\n",
    "        if len(row['Respuesta']) == 1:\n",
    "            row['Respuesta'] = row['Respuesta'][0]\n",
    "            return row\n",
    "        else:\n",
    "            area = row['Area']\n",
    "            dimension = row['Dimension']\n",
    "            for sent in row['Respuesta']:\n",
    "                tempArray = []\n",
    "                tempArray.append(area)    \n",
    "                tempArray.append(dimension)\n",
    "                tempArray.append(sent)\n",
    "                arrayExterno.append(tempArray)\n",
    "            row['Respuesta'] = \"\"\n",
    "            return row\n",
    "    else:\n",
    "        return row\n",
    "\n",
    "dfSntTok = dfSntTok.apply(separeSentences, axis = 1).copy()\n",
    "dfSntTok = pd.concat([dfSntTok,\\\n",
    "           pd.DataFrame(arrayExterno, columns=[\"Area\", \"Dimension\", \"Respuesta\"])],ignore_index=True).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeText(tokens):\n",
    "    tokens = tokens.replace(r'_', r'').replace(r'¨¢', r'a').replace(r'¨¦',r'e').replace(r'¨ª', r'i')\\\n",
    "        .replace(r'¨²', r'u').replace(r'¨®', r'o').replace(r'?', r'n').replace(r'á', r'a')\\\n",
    "        .replace(r'é',r'e').replace(r'í', r'i').replace(r'ó', r'o').replace(r'ú', r'u').replace(r'ñ', r'n')\\\n",
    "        .replace('Á', r'A').replace(r'É',r'E').replace(r'Í', r'I').replace(r'Ó', r'O').replace(r'Ú', r'U')\\\n",
    "        .replace(r'Ñ', r'N')\n",
    "    tokens = re.sub(r'[^\\w\\s ÑñáéíóúÁÉÍÓÚ]', '', tokens)\n",
    "    tokens = re.sub('\\d','',tokens)\n",
    "    return tokens.lower()\n",
    "\n",
    "stop = stopwords.words('spanish')\n",
    "def removeStopWords(tokens, stopword):\n",
    "    new_ = []\n",
    "    for token in tokens:\n",
    "        if token not in stopword and token not in ['etc']:\n",
    "            new_.append(token)\n",
    "    return new_\n",
    "\n",
    "dfNorm = dfSntTok.copy()\n",
    "dfNorm[\"Respuesta\"] = dfNorm[\"Respuesta\"].apply(normalizeText)\\\n",
    "                                         .apply(nltk.word_tokenize)\\\n",
    "                                         .apply(removeStopWords, stopword = stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    165450.000000\n",
       "mean     6.573460     \n",
       "std      2.213859     \n",
       "min      4.000000     \n",
       "25%      5.000000     \n",
       "50%      6.000000     \n",
       "75%      9.000000     \n",
       "max      10.000000    \n",
       "Name: Respuesta, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmaDiccionario = {}\n",
    "with open('Herramientas/lemma.txt', 'rb') as fichero:\n",
    "    datos = (fichero.read().decode('utf8').replace(u'\\r', u'').split(u'\\n'))\n",
    "    datos = ([avance.split(u'\\t') for avance in datos])\n",
    "for avance in datos:\n",
    "   if len(avance) >1:\n",
    "      lemmaDiccionario[avance[1]] = avance[0]\n",
    "        \n",
    "def lemmatize(word):\n",
    "   return lemmaDiccionario.get(word, word + u'')\n",
    "   \n",
    "def lemmatize_words(words):\n",
    "    new_words = []\n",
    "    for palabra in words:\n",
    "        new_word = lemmatize(palabra)\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "dfNorm = dfNorm[dfNorm['Respuesta'].map(len) > 3]\n",
    "dfNorm[\"Respuesta\"] = dfNorm[\"Respuesta\"].apply(lemmatize_words).apply(lambda x: x if len(x) < 10 else x[:10])\n",
    "dfNorm[\"Respuesta\"].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x1ba7c143848>,\n",
       "  <matplotlib.lines.Line2D at 0x1ba7c143048>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x1ba7c143708>,\n",
       "  <matplotlib.lines.Line2D at 0x1ba7c13b2c8>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x1ba7c1403c8>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x1ba7c13b1c8>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x1ba7c13b108>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplots()\n",
    "dataToPlot = [dfNorm[\"Respuesta\"].apply(len)]\n",
    "plt.boxplot(dataToPlot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_quantity = max(dfNorm.groupby(['Area','Dimension']).count().Respuesta)\n",
    "array_groups_to_remove = []\n",
    "for name, group in dfNorm.groupby(['Area','Dimension']):\n",
    "    if len(group) <= max_quantity/10:\n",
    "        array_groups_to_remove.append(name)\n",
    "\n",
    "dfNorm = dfNorm.apply(lambda x, arr_rem: None if (x['Area'],x['Dimension']) in arr_rem else x, axis = 1\\\n",
    "                      , arr_rem = array_groups_to_remove).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Area                  Dimension                   \n",
       "Confianza General     Confianza General               11901\n",
       "Gobierno Corporativo  Anti-corrupción                 7004 \n",
       "                      Integridad                      7186 \n",
       "Medio Ambiente        Ambiental                       2137 \n",
       "                      Cultura Ambiental               10875\n",
       "Social Externo        Calidad                         6744 \n",
       "                      Cliente                         5811 \n",
       "                      Comunicación                    3195 \n",
       "                      Comunidad                       4939 \n",
       "                      Educación                       1928 \n",
       "                      Imagen                          6414 \n",
       "                      Infraestructura                 2055 \n",
       "                      Operación                       2079 \n",
       "                      Proveedores                     2078 \n",
       "                      Reclamos                        5865 \n",
       "                      Recompra                        16273\n",
       "                      Salud & Seguridad               4497 \n",
       "                      Trabajo                         1985 \n",
       "                      Valor                           6175 \n",
       "Social Interno        Compromiso                      5571 \n",
       "                      Comunicación                    5676 \n",
       "                      Diversidad e Inclusión          2580 \n",
       "                      Empleados                       10057\n",
       "                      Entrenamiento y Desarrollo      6084 \n",
       "                      Equilibrio Vida-Trabajo         5766 \n",
       "                      Estructura de Compensaciones    6735 \n",
       "                      Identidad Cultural              2579 \n",
       "                      Salud & Seguridad               6330 \n",
       "Name: Respuesta, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNorm.groupby(['Area','Dimension']).count().Respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNorm.to_csv(\"Data/Flujo1.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stemmer_spanish = SnowballStemmer('spanish')\n",
    "def stemWordsArray(tokens, stemmer):\n",
    "    new_ = []\n",
    "    for token in tokens:\n",
    "        new_.append(stemmer.stem(token))\n",
    "    return new_\n",
    "\n",
    "dfStemmer = dfNorm.copy()\n",
    "dfStemmer['Respuesta'] = dfStemmer['Respuesta'].apply(stemWordsArray, stemmer = Stemmer_spanish)\n",
    "dfStemmer.to_csv(\"Data/Flujo2.csv\", sep=\";\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
