{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import pandas as pd\n",
    "import os\n",
    "from unicodedata import normalize\n",
    "import pandas as pd\n",
    "import nltk.data\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import num2words\n",
    "from num2words import num2words\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from textblob import TextBlob\n",
    "\n",
    "arrayHeaders = [\"Area\", \"Dimension\", \"Respuesta\"]\n",
    "data = pd.read_csv(\"Data/example.csv\", header= None, names = arrayHeaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayAreas= []\n",
    "for area in data[\"Area\"]:\n",
    "    area = area.split('\\n')\n",
    "    area = area[2]\n",
    "    area = area.replace(':es: ','')\n",
    "    area = area.replace('\"','')\n",
    "    area = area.replace('\\r','')\n",
    "    arrayAreas.append(area)\n",
    "arrayAreas = pd.Series(arrayAreas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayDimensiones = []\n",
    "for dimension in data[\"Dimension\"]:\n",
    "    dimension = dimension.split('\\n')\n",
    "    dimension = dimension[2]\n",
    "    dimension = dimension.replace(':es: ','')\n",
    "    dimension = dimension.replace('\"','')\n",
    "    dimension = dimension.replace('\\r','')\n",
    "    arrayDimensiones.append(dimension)\n",
    "    \n",
    "arrayDimensiones = pd.Series(arrayDimensiones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "posAnswers = []\n",
    "negAnswers = []\n",
    "\n",
    "for resp in data[\"Respuesta\"]:\n",
    "    resp = resp.split('\\n')\n",
    "    resp_pos = resp[2].replace('       positive: ','')\n",
    "    resp_pos = resp_pos.replace(\"'\",'')\n",
    "    resp_pos = resp_pos.replace('\\r','')\n",
    "    posAnswers.append(resp_pos)\n",
    "    if len(resp)==4:\n",
    "        resp_neg = resp[3].replace('       negative: ','')\n",
    "        resp_neg = resp_neg.replace(\"'\",'')\n",
    "        resp_neg = resp_neg.replace('\\r','')\n",
    "        negAnswers.append(resp_neg)\n",
    "    else:\n",
    "        negAnswers.append('')\n",
    "        \n",
    "posAnswers = pd.Series(posAnswers)\n",
    "negAnswers = pd.Series(negAnswers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "concatenaci贸nPositivas = pd.concat([arrayAreas,arrayDimensiones, posAnswers], axis = 1)\n",
    "concatenaci贸nNegativas = pd.concat([arrayAreas,arrayDimensiones, negAnswers], axis = 1)\n",
    "df = pd.concat([concatenaci贸nPositivas,concatenaci贸nNegativas],ignore_index=True).rename(columns = pd.Series(arrayHeaders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def remplaceSimbols(token):\n",
    "    token = re.sub(r'[^\\w\\s]', ' ', token)\n",
    "    return token\n",
    "\n",
    "df[\"Respuesta\"] = df[\"Respuesta\"].apply(remplaceSimbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTestTokenized = df.copy()\n",
    "dataTestTokenized[\"Respuesta\"] = dataTestTokenized[\"Respuesta\"].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = []\n",
    "stop = stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifyStopWords(tokens):\n",
    "    for token in tokens:\n",
    "        if token.lower() in stop:\n",
    "            tokens.pop(tokens.index(token))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataTestStop = dataTestTokenized.copy()\n",
    "dataTestStop[\"Respuesta\"] = dataTestStop[\"Respuesta\"].apply(verifyStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMoreWords = dataTestStop.copy()\n",
    "dfMoreWords = dfMoreWords[dfMoreWords['Respuesta'].map(len) > 3].copy()\n",
    "dfMoreWords = dfMoreWords.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  verifyNormalize(tokens):\n",
    "    for token in tokens:\n",
    "        if token.isdigit()== False:\n",
    "            token2 = re.sub(r'[^\\w\\s]', ' ', token)\n",
    "            if token != token2:\n",
    "                print(token+\" pasa a \"+token2)\n",
    "    return tokens\n",
    "\n",
    "def normalizeTokens(tokens):\n",
    "    for token in tokens:\n",
    "        pos = tokens.index(token)\n",
    "        if token.isdigit():\n",
    "            tokens[pos] = num2words(token, lang='es')\n",
    "        else:\n",
    "            token = re.sub(r'[^\\w\\s]', ' ', token)\n",
    "            if token != ' ' or token != '':\n",
    "                tokens[pos] = token.lower()\n",
    "                #tokens[pos] = unicodedata.normalize('NFKD', token.lower()).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            else:\n",
    "                tokens.pop(pos)\n",
    "    return tokens\n",
    "\n",
    "def verifyLang(tokens):\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if token.isdigit() == False:\n",
    "            b = TextBlob(token)\n",
    "            if b.detect_language() == \"es\":\n",
    "                count = count + 1\n",
    "    if len(tokens)-2 < count:\n",
    "        return ''\n",
    "    else:\n",
    "        return tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNormalized = dfMoreWords.copy()\n",
    "dfNormalized[\"Respuesta\"] = dfNormalized['Respuesta'].apply(normalizeTokens)\n",
    "\n",
    "DataFinalFlujo1 = dfNormalized.copy()\n",
    "DataFinalFlujo1[\"Respuesta\"] = DataFinalFlujo1[\"Respuesta\"].apply(verifyStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmaDiccionario = {}\n",
    "with open('Herramientas/lemma.txt', 'rb') as fichero:\n",
    "    datos = (fichero.read().decode('utf8').replace(u'\\r', u'').split(u'\\n'))\n",
    "    datos = ([avance.split(u'\\t') for avance in datos])\n",
    "for avance in datos:\n",
    "   if len(avance) >1:\n",
    "      lemmaDiccionario[avance[1]] = avance[0]\n",
    "        \n",
    "def lemmatize(word):\n",
    "   return lemmaDiccionario.get(word, word + u'')\n",
    "   \n",
    "def lemmatize_words(words):\n",
    "    new_words = []\n",
    "    for palabra in words:\n",
    "        new_word = lemmatize(palabra)\n",
    "        new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFinalFlujo1[\"Respuesta\"] = DataFinalFlujo1[\"Respuesta\"].apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Fin Flujo 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "Stemmer_spanish = SnowballStemmer('spanish')\n",
    "def stemWordsArray(tokens):\n",
    "    for token in tokens:\n",
    "        tokens[tokens.index(token)] = Stemmer_spanish.stem(token)\n",
    "    return tokens\n",
    "\n",
    "DataFinalFlujo2 = DataFinalFlujo1.copy()\n",
    "DataFinalFlujo2['Respuesta'] = DataFinalFlujo2['Respuesta'].apply(stemWordsArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFinalFlujo2.to_csv(\"Data/Flujo2.csv\", sep=\";\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
