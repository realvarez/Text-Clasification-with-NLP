{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "import nltk\n",
    "import boto3\n",
    "import pandas as uwu\n",
    "import json\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import model_from_yaml\n",
    "\n",
    "def remove_stop_words(list_tokens):\n",
    "    stop = stopwords.words('spanish')\n",
    "    for token in list_tokens:\n",
    "        if token.lower() in stop:\n",
    "            list_tokens.pop(list_tokens.index(token))\n",
    "    return list_tokens\n",
    "\n",
    "\n",
    "def remove_void_elements(tokens):\n",
    "    for token in tokens:\n",
    "        if token == '' or token == \" \" or token == \"  \":\n",
    "            tokens.pop(tokens.index(token))\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def normalize_text(tokens):\n",
    "    for token in tokens:\n",
    "        pos = tokens.index(token)\n",
    "        if token.isdigit() or token.isalpha() == False:\n",
    "            tokens.pop(pos)\n",
    "        else:\n",
    "            token = re.sub(r'[^\\w\\s]', ' ', token)\n",
    "            if token != ' ' or token != '':\n",
    "                tokens[pos] = unicodedata.normalize('NFKD', token.lower()).encode('ascii', 'ignore').decode('utf-8','ignore')\n",
    "            else:\n",
    "                tokens.pop(pos)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def create_lemma_dict():\n",
    "    lemmaDiccionario = {}\n",
    "    with open('s3://titleproyectbucket2019/Herramientas/lemma.txt', 'rb') as fichero:\n",
    "        datos = (fichero.read().decode('utf8').replace(u'\\r', u'').split(u'\\n'))\n",
    "        datos = ([avance.split(u'\\t') for avance in datos])\n",
    "    for avance in datos:\n",
    "        if len(avance) > 1:\n",
    "            lemmaDiccionario[avance[1]] = avance[0]\n",
    "\n",
    "    return lemmaDiccionario\n",
    "\n",
    "\n",
    "def lemmatize(lemmaDiccionario, word):\n",
    "    return lemmaDiccionario.get(word, word + u'')\n",
    "\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    lemmaDiccionario = create_lemma_dict()\n",
    "    new_words = []\n",
    "    for palabra in words:\n",
    "        new_word = lemmatize(lemmaDiccionario, palabra)\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def wordsToNumbers(tokens, vocabulary):\n",
    "    number_array = []\n",
    "    for i in tokens:\n",
    "        number_array.append(vocabulary.index(i) + 1)\n",
    "    return np.asarray(number_array)\n",
    "\n",
    "\n",
    "def word_to_vec(array_text, model):\n",
    "    model = Word2Vec.load('modelWord2vec.bin')\n",
    "    array_vectors = []\n",
    "    for word in array_text:\n",
    "        array_vectors.append(model.wv['word'])\n",
    "    return array_vectors\n",
    "\n",
    "\n",
    "def processText(Text):\n",
    "    list_text = nltk.word_tokenize(Text)\n",
    "    list_text = remove_stop_words(list_text)\n",
    "    list_text = remove_void_elements(list_text)\n",
    "    list_text = normalize_text(list_text)\n",
    "    list_text = lemmatize_words(list_text)\n",
    "    if len(list_text) > 10:\n",
    "        return list_text[:10]\n",
    "    else:\n",
    "        return list_text\n",
    "\n",
    "def downloadFiles(area):\n",
    "    s3 = boto3.client('s3')\n",
    "    #downloadingFiles\n",
    "    s3.download_file('titleproyectbucket2019', 'vocabulary.csv', '/tmp/vocabulary.csv')\n",
    "    s3.download_file('titleproyectbucket2019', 'Herramientas/lemma.txt', '/tmp/lemma.txt')\n",
    "    #downloadingModels\n",
    "    if area=='Medio Ambiente':\n",
    "        s3.download_file('titleproyectbucket2019', '/Models/MA/modelMA.yaml', '/tmp/modelMA.yaml')\n",
    "        s3.download_file('titleproyectbucket2019', '/Models/MA/modelMA.h5', '/tmp/modelMA.h5')\n",
    "    elif area=='Gobierno Corporativo':\n",
    "        s3.download_file('titleproyectbucket2019', '/Models/GOB/modelGOB.yaml', '/tmp/modelGOB.yaml')\n",
    "        s3.download_file('titleproyectbucket2019', '/Models/GOB/modelGOB.h5', '/tmp/modelGOB.h5')\n",
    "    elif area=='Social Externo':\n",
    "        s3.download_file('titleproyectbucket2019', '/Models/SEXT/modelSEXT.yaml', '/tmp/modelSEXT.yaml')\n",
    "        s3.download_file('titleproyectbucket2019', '/Models/SEXT/modelSEXT.h5', '/tmp/modelSEXT.h5')\n",
    "    elif area == 'Social Interno':\n",
    "        s3.download_file('titleproyectbucket2019', '/Models/SINT/modelSINT.yaml', '/tmp/modelSINT.yaml')\n",
    "        s3.download_file('titleproyectbucket2019', '/Models/SINT/modelSINT.h5', '/tmp/modelSINT.h5')\n",
    "\n",
    "def get_dimension_of_text(text, area):\n",
    "    if not isinstance(text, str) or len(text) < 5:\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps('Texto a clasificar mal ingresado o muy corto (minimo 5 palabras)')\n",
    "        }\n",
    "    if area not in ['Gobierno Corporativo', 'Medio Ambiente', 'Social Externo', 'Social Interno']:\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps('El area ingresada no coincide, puede ser Gobierno Corporativo, Medio Ambiente, Social Externo o Social Interno')\n",
    "        }\n",
    "    list_words = processText(text)\n",
    "    downloadFiles(area)\n",
    "    vocab = set(uwu.readcsv('s3://titleproyectbucket2019/vocabulary.csv'))\n",
    "    list_words_numeric = wordsToNumbers(list_words, vocab)\n",
    "    list_words_numeric = pad_sequences(list_words_numeric, maxlen=10, dtype='object', padding='post', value=0)\n",
    "    if area == 'Gobierno Corporativo':\n",
    "        yaml_file = open('/tmp/modelGOB.yaml', 'r')\n",
    "        loaded_model_yaml = yaml_file.read()\n",
    "        yaml_file.close()\n",
    "        RNNmodel = model_from_yaml(loaded_model_yaml)\n",
    "        RNNmodel.load_weights(\"/tmp/modelGOB.h5\")\n",
    "        return RNNmodel.predict(list_words_numeric)\n",
    "\n",
    "    elif area == 'Medio Ambiente':\n",
    "        yaml_file = open('/tmp/modelMA.yaml', 'r')\n",
    "        loaded_model_yaml = yaml_file.read()\n",
    "        yaml_file.close()\n",
    "        RNNmodel = model_from_yaml(loaded_model_yaml)\n",
    "        RNNmodel.load_weights(\"/tmp/modelMA.h5\")\n",
    "        return RNNmodel.predict(list_words_numeric)\n",
    "\n",
    "    elif area == 'Social Externo':\n",
    "        yaml_file = open('/tmp/modelSEXT.yaml', 'r')\n",
    "        loaded_model_yaml = yaml_file.read()\n",
    "        yaml_file.close()\n",
    "        RNNmodel = model_from_yaml(loaded_model_yaml)\n",
    "        RNNmodel.load_weights(\"/tmp/modelSEXT.h5\")\n",
    "        return RNNmodel.predict(list_words_numeric)\n",
    "\n",
    "    elif area == 'Social Interno':\n",
    "        yaml_file = open('/tmp/modelSINT.yaml', 'r')\n",
    "        loaded_model_yaml = yaml_file.read()\n",
    "        yaml_file.close()\n",
    "        RNNmodel = model_from_yaml(loaded_model_yaml)\n",
    "        RNNmodel.load_weights(\"/tmp/modelSINT.h5\")\n",
    "        return RNNmodel.predict(list_words_numeric)\n",
    "\n",
    "\n",
    "\n",
    "def lambda_handler_make_process(event, context):\n",
    "    body = json.loads(event['body'])\n",
    "    area = body['area']\n",
    "    text = body['text']\n",
    "    prediction = get_dimension_of_text(text, area)\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('La predicción es '+prediction)\n",
    "    }\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    s3 = boto3.client('s3')\n",
    "    body = json.loads(event['body'])\n",
    "    area = body['area']\n",
    "    text = body['text']\n",
    "\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps('Estas consultando por area:'+area+' para clasificar texto:'+text)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Usuario/nltk_data'\n    - 'D:\\\\ProgramasVarios\\\\Anaconda\\\\nltk_data'\n    - 'D:\\\\ProgramasVarios\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'D:\\\\ProgramasVarios\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0cc113754392>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_dimension_of_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'creo que esta mal la empreza, muchos desastres, basura, no enseñan nada ni reciclan'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Medio Ambiente'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-856902609319>\u001b[0m in \u001b[0;36mget_dimension_of_text\u001b[1;34m(text, area)\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;34m'body'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'El area ingresada no coincide, puede ser Gobierno Corporativo, Medio Ambiente, Social Externo o Social Interno'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         }\n\u001b[1;32m--> 122\u001b[1;33m     \u001b[0mlist_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m     \u001b[0mdownloadFiles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marea\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muwu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m's3://titleproyectbucket2019/vocabulary.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-856902609319>\u001b[0m in \u001b[0;36mprocessText\u001b[1;34m(Text)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprocessText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mText\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[0mlist_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mText\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[0mlist_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_stop_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mlist_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mremove_void_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramasVarios\\Anaconda\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \"\"\"\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     return [\n\u001b[0;32m    146\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramasVarios\\Anaconda\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramasVarios\\Anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramasVarios\\Anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramasVarios\\Anaconda\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Usuario/nltk_data'\n    - 'D:\\\\ProgramasVarios\\\\Anaconda\\\\nltk_data'\n    - 'D:\\\\ProgramasVarios\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'D:\\\\ProgramasVarios\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "get_dimension_of_text('creo que esta mal la empreza, muchos desastres, basura, no enseñan nada ni reciclan', 'Medio Ambiente')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
