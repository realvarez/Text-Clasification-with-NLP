{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "# cargar doc en memoria\n",
    "def load_doc(filename):\n",
    " # abrir el archivo como de sólo lectura\n",
    " file = open(filename, 'r')\n",
    " # leer todo el texto\n",
    " text = file.read()\n",
    " # cerrar el archivo\n",
    " file.close()\n",
    " return text\n",
    "# convertir doc en tokens limpios\n",
    "def clean_doc(doc, vocab):\n",
    " # dividido en tokens por espacio en blanco\n",
    " tokens = doc.split()\n",
    " # prepare a regex para el filtrado de caracteres\n",
    " re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    " # eliminar la puntuación de cada palabra\n",
    " tokens = [re_punc.sub('', w) for w in tokens]\n",
    " # filtrar las tokens que no están en el vocabulario\n",
    " tokens = [w for w in tokens if w in vocab]\n",
    " tokens = ' '.join(tokens)\n",
    " return tokens\n",
    "# cargar todos los documentos en un directorio\n",
    "def process_docs(directory, vocab, is_train):\n",
    " documents = list()\n",
    " # revisar todos los archivos de la carpeta\n",
    " for filename in listdir(directory):\n",
    "  # omita cualquier comentario en el juego de pruebas\n",
    "  if is_train and filename.startswith('cv9'):\n",
    "   continue\n",
    "  if not is_train and not filename.startswith('cv9'):\n",
    "   continue\n",
    "  # crear la ruta completa del archivo a abrir\n",
    "  path = directory + '/' + filename\n",
    "  # cargar el documento\n",
    "  doc = load_doc(path)\n",
    "  # expediente limpio\n",
    "  tokens = clean_doc(doc, vocab)\n",
    "  # añadir a la lista\n",
    "  documents.append(tokens)\n",
    " return documents\n",
    "# cargar y limpiar un conjunto de datos\n",
    "def load_clean_dataset(vocab, is_train):\n",
    " # cargar documentos\n",
    " neg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
    " pos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
    " docs = neg + pos\n",
    " # preparar las etiquetas\n",
    " labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    " return docs, labels\n",
    "# instalar un tokenizador\n",
    "def create_tokenizer(lines):\n",
    " tokenizer = Tokenizer()\n",
    " tokenizer.fit_on_texts(lines)\n",
    " return tokenizer\n",
    "# codificar y rellenar documentos enteros\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    " # codificación entera\n",
    " encoded = tokenizer.texts_to_sequences(docs)\n",
    " # secuencias de pads\n",
    " padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    " return padded\n",
    "# definir el modelo\n",
    "def define_model(vocab_size, max_length):\n",
    " model = Sequential()\n",
    " model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    " model.add(Conv1D(filters=32, kernel_size=8, activation='relu'))\n",
    " model.add(MaxPooling1D(pool_size=2))\n",
    " model.add(Flatten())\n",
    " model.add(Dense(10, activation='relu'))\n",
    " model.add(Dense(1, activation='sigmoid'))\n",
    " # compilar red\n",
    " model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " # resumir el modelo definido\n",
    " model.summary()\n",
    " plot_model(model, to_file='model.png', show_shapes=True)\n",
    " return model\n",
    "\n",
    "\n",
    "# cargar el vocabulario\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "# cargar datos de entrenamiento\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "# crear el tokenizador\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# definir el tamaño del vocabulario\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Tamaño del vocabulario: %d' % vocab_size)\n",
    "# calcular la longitud máxima de la secuencia\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "print('Longitud Maxima: %d' % max_length)\n",
    "# datos codificados\n",
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "# definir modelo\n",
    "model = define_model(vocab_size, max_length)\n",
    "# red adecuada\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "# guardar el modelo\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
