{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1655,
     "status": "ok",
     "timestamp": 1578783714414,
     "user": {
      "displayName": "Ricardo Alvarez",
      "photoUrl": "",
      "userId": "07173409030660719160"
     },
     "user_tz": 180
    },
    "id": "1Gi7fjX8LhCU",
    "outputId": "68719163-75f0-4b84-cd0b-49eb880591ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import ast, os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import plot_confusion_matrix, precision_recall_fscore_support\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import model_from_yaml\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a941CK-3L00A"
   },
   "outputs": [],
   "source": [
    "if os.getcwd()[0] == 'C':\n",
    "    df = pd.read_csv('../Data/Flujo1.csv', sep=\";\")\n",
    "else:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/')\n",
    "    drive_route = 'drive/My Drive/Tesis/Data/'\n",
    "    df = pd.read_csv(drive_route+'/Flujo1.csv', sep=\";\")\n",
    "df['Respuesta'] = df['Respuesta'].apply(ast.literal_eval)\n",
    "\n",
    "def enumerate_dimensions(dimension, list_dimensions): \n",
    "    return list_dimensions.index(dimension)\n",
    "def wordsToNumbers(tokens, vocabulary):\n",
    "    number_array = []\n",
    "    for i in tokens:\n",
    "        number_array.append(vocabulary.index(i)+1)\n",
    "    return np.asarray(number_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media 5708.666666666667\n",
      "STD 2110.6862496459403\n",
      "Cant. datos 51378\n",
      "Cant. datos 2579\n",
      "Cant. datos min  2579\n",
      "Cant. datos max  10057\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Dimension</th>\n",
       "      <th>Respuesta</th>\n",
       "      <th>NumRespuesta</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_dimension</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5676</td>\n",
       "      <td>5676</td>\n",
       "      <td>5676</td>\n",
       "      <td>5676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6735</td>\n",
       "      <td>6735</td>\n",
       "      <td>6735</td>\n",
       "      <td>6735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2579</td>\n",
       "      <td>2579</td>\n",
       "      <td>2579</td>\n",
       "      <td>2579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6330</td>\n",
       "      <td>6330</td>\n",
       "      <td>6330</td>\n",
       "      <td>6330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2580</td>\n",
       "      <td>2580</td>\n",
       "      <td>2580</td>\n",
       "      <td>2580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10057</td>\n",
       "      <td>10057</td>\n",
       "      <td>10057</td>\n",
       "      <td>10057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6084</td>\n",
       "      <td>6084</td>\n",
       "      <td>6084</td>\n",
       "      <td>6084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5571</td>\n",
       "      <td>5571</td>\n",
       "      <td>5571</td>\n",
       "      <td>5571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5766</td>\n",
       "      <td>5766</td>\n",
       "      <td>5766</td>\n",
       "      <td>5766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Area  Dimension  Respuesta  NumRespuesta\n",
       "_dimension                                           \n",
       "0            5676       5676       5676          5676\n",
       "1            6735       6735       6735          6735\n",
       "2            2579       2579       2579          2579\n",
       "3            6330       6330       6330          6330\n",
       "4            2580       2580       2580          2580\n",
       "5           10057      10057      10057         10057\n",
       "6            6084       6084       6084          6084\n",
       "7            5571       5571       5571          5571\n",
       "8            5766       5766       5766          5766"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se Construye Dataframe solo de medio ambiente\n",
    "df = df[df.Area == 'Social Interno']\n",
    "# Lista de dimensiones se pasa a numeros\n",
    "list_dimensions = df.Dimension.unique().tolist()\n",
    "num_classes = len(list_dimensions)\n",
    "df['_dimension'] = df['Dimension'].apply(enumerate_dimensions, list_dimensions = list_dimensions)\n",
    "\n",
    "#Generamos un vocabulario de palabras\n",
    "vocabulary = []\n",
    "df['Respuesta'].apply(vocabulary.append)\n",
    "vocabulary = [item for sublist in vocabulary for item in sublist]\n",
    "vocabulary.append('')\n",
    "vocabulary = list(sorted(set(vocabulary)))\n",
    "vocab_len= len(vocabulary)+1\n",
    "\n",
    "df['NumRespuesta'] = df.Respuesta.apply (wordsToNumbers, vocabulary = vocabulary)\n",
    "\n",
    "# Rellenamos las matrices con 0 para que todas tengan el mismo tama√±o\n",
    "X_completo = pad_sequences(df.NumRespuesta, maxlen=10, dtype='object', padding='post', value = 0)\n",
    "y_completo = np.array(df['_dimension'])\n",
    "\n",
    "print('Media', np.array(df.groupby('_dimension').count().NumRespuesta).mean())\n",
    "print('STD', np.std(df.groupby('_dimension').count().NumRespuesta))\n",
    "print('Cant. datos', np.sum(df.groupby('_dimension').count().NumRespuesta))\n",
    "print('Cant. datos', np.min(df.groupby('_dimension').count().NumRespuesta))\n",
    "print('Cant. datos min ', np.min(df.groupby('_dimension').count().NumRespuesta))\n",
    "print('Cant. datos max ', np.max(df.groupby('_dimension').count().NumRespuesta))\n",
    "\n",
    "df.groupby('_dimension').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Busqueda de Hyperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_train_model(tensor_X, tensor_y, tensor_test_x, tensor_test_y, paramsEmbedding, paramsLSTM, num_clases, optimizer, batch_size, epoch):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(input_dim = paramsEmbedding['input_dim'],  output_dim = paramsEmbedding['output_dim'], input_shape=(tensor_X.shape[1],)))\n",
    "    model.add(tf.keras.layers.LSTM(units = paramsLSTM['units'],\n",
    "                                   activation = paramsLSTM['activation'],\n",
    "                                   dropout = paramsLSTM['dropout'],\n",
    "                                   recurrent_dropout = paramsLSTM['recurrent_dropout']))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units = num_clases ,activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "    \n",
    "    history = model.fit(tensor_X,\n",
    "                        tensor_y,\n",
    "                        epochs = epoch,\n",
    "                        batch_size = batch_size,\n",
    "                        verbose = 0\n",
    "                       )\n",
    "    test_loss, test_acurracy = model.evaluate(tensor_test_x,\n",
    "                                              tensor_test_y,\n",
    "                                              verbose=0\n",
    "                                             )\n",
    "    \n",
    "    predictions = model.predict_classes(tensor_test_x)\n",
    "    medidas = precision_recall_fscore_support(tensor_test_y, predictions )\n",
    "    print(medidas[0])\n",
    "    return [test_acurracy, test_loss, medidas, paramsEmbedding, paramsLSTM, optimizer, batch_size, epoch]\n",
    "\n",
    "\n",
    "def grid_lstm(data_x, data_y, input_dim, output_dim_embe, units_lstm, activ_lstm, drop_lstm, rec_drop_lstm, num_clases, optimizers, batch_size_, epochs):\n",
    "    X, test_x, y, test_y = train_test_split(data_x, data_y, test_size = 0.1, random_state = 0)\n",
    "    tensor_X      = tf.convert_to_tensor(list(X))\n",
    "    tensor_test_x = tf.convert_to_tensor(list(test_x))\n",
    "    tensor_y      = tf.convert_to_tensor(list(y))\n",
    "    tensor_test_y = tf.convert_to_tensor(list(test_y))\n",
    "    resultados = []\n",
    "    for output_dim in output_dim_embe:\n",
    "        for units in units_lstm:\n",
    "            for activ in activ_lstm:\n",
    "                for drop in drop_lstm:\n",
    "                    for rec_drop in rec_drop_lstm:\n",
    "                        for optimizer in optimizers:\n",
    "                            for batch_size in batch_size_:\n",
    "                                for epoch in epochs:\n",
    "                                    resultados.append( gen_train_model(tensor_X, tensor_y, tensor_test_x, tensor_test_y,  {'input_dim': input_dim, 'output_dim':output_dim}, \n",
    "                                                    {'units':units,'activation':activ,'dropout':drop, 'recurrent_dropout':rec_drop},\n",
    "                                                   num_clases, optimizer, batch_size, epoch))\n",
    "    return resultados\n",
    "\n",
    "output = grid_lstm(data_x = X_completo,\n",
    "          data_y = y_completo,\n",
    "          input_dim = len(vocabulary)+1,\n",
    "          num_clases = len(list_dimensions),\n",
    "          output_dim_embe = [60, 90],\n",
    "          units_lstm = [60, 90],\n",
    "          activ_lstm = ['tanh', 'relu'],\n",
    "          drop_lstm = [0.3,0.5,0.7],\n",
    "          rec_drop_lstm = [0.3,0.5,0.7],\n",
    "          optimizers = ['rmsprop','adam'],\n",
    "          batch_size_ = [20, 40],\n",
    "          epochs = [6,8]\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-282ef70f0793>:48: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "epoch = 6\n",
    "batch_size = 40\n",
    "optimizer = 'rmsprop'\n",
    "recurrent_dropout = 0.7\n",
    "dropout = 0.3\n",
    "activation_lstm = 'relu'\n",
    "lstm_units = 90\n",
    "ouput_dim_embedding = 90\n",
    "\n",
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 2)\n",
    "resultados = []\n",
    "contador = 1\n",
    "for valores_entrenamiento, valores_testeo in kf.split(X_completo):\n",
    "    \n",
    "    tensor_X      = tf.convert_to_tensor(list(X_completo[valores_entrenamiento]))\n",
    "    tensor_test_x = tf.convert_to_tensor(list(X_completo[valores_testeo]))\n",
    "    tensor_y      = tf.convert_to_tensor(list(y_completo[valores_entrenamiento]))\n",
    "    tensor_test_y = tf.convert_to_tensor(list(y_completo[valores_testeo]))\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Embedding(input_dim = vocab_len, \n",
    "                                        output_dim = ouput_dim_embedding,\n",
    "                                        input_shape=(X_completo.shape[1],)))\n",
    "    \n",
    "    model.add(tf.keras.layers.LSTM(units=lstm_units,\n",
    "                                   activation=activation_lstm,\n",
    "                                   dropout = dropout,\n",
    "                                   recurrent_dropout = recurrent_dropout))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(units=num_classes,\n",
    "                                    activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer = optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['sparse_categorical_accuracy'])\n",
    "    \n",
    "    history = model.fit(tensor_X,\n",
    "                        tensor_y,\n",
    "                        epochs = epoch,\n",
    "                        batch_size = batch_size,\n",
    "                        verbose=0\n",
    "                       )\n",
    "    \n",
    "    test_loss, test_acurracy = model.evaluate(tensor_test_x,\n",
    "                                              tensor_test_y,\n",
    "                                              verbose=0\n",
    "                                             )\n",
    "    predictions = model.predict_classes(tensor_test_x)\n",
    "    medidas = precision_recall_fscore_support(tensor_test_y, predictions)\n",
    "    resultados.append([test_loss, test_acurracy, medidas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"loss mean\",np.array([res[0] for res in resultados]).mean())\n",
    "print(\"acurracy mean\",np.array([res[1] for res in resultados]).mean())\n",
    "print(\"media precision\",np.array([np.array([res[2][0][i] for res in resultados]).mean() for i in range(num_classes)]).mean())\n",
    "print(\"media recall\",np.array([np.array([res[2][1][i] for res in resultados]).mean() for i in range(num_classes)]).mean())\n",
    "print(\"media fscore\",np.array([np.array([res[2][2][i] for res in resultados]).mean() for i in range(num_classes)]).mean())\n",
    "print(\"media support\",np.array([np.array([res[2][3][i] for res in resultados]).mean() for i in range(num_classes)]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('complete_social_ext.h5')\n",
    "model_ = tf.keras.models.load_model('complete_social_ext.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "history_dict = history.history\n",
    "acc = history_dict['sparse_categorical_accuracy']\n",
    "loss = history_dict['loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()   # clear figure\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Falta ROC Multiclase\n",
    "import sklearn.metrics as metrics\n",
    "fpr, tpr, threshold = metrics.roc_curve(tensor_test_y, predictions)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Antiguo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 67079,
     "status": "ok",
     "timestamp": 1578786739655,
     "user": {
      "displayName": "Ricardo Alvarez",
      "photoUrl": "",
      "userId": "07173409030660719160"
     },
     "user_tz": 180
    },
    "id": "VDP2y1ZnW_0A",
    "outputId": "cbd334df-6602-4465-fc71-6ca1d682a2cf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 41073 samples\n",
      "Epoch 1/7\n",
      "41073/41073 [==============================] - 9s 211us/sample - loss: 2.0185 - sparse_categorical_accuracy: 0.2403\n",
      "Epoch 2/7\n",
      "41073/41073 [==============================] - 8s 194us/sample - loss: 1.1987 - sparse_categorical_accuracy: 0.6281\n",
      "Epoch 3/7\n",
      "41073/41073 [==============================] - 8s 191us/sample - loss: 0.8866 - sparse_categorical_accuracy: 0.7326\n",
      "Epoch 4/7\n",
      "41073/41073 [==============================] - 8s 191us/sample - loss: 0.7618 - sparse_categorical_accuracy: 0.7712\n",
      "Epoch 5/7\n",
      "41073/41073 [==============================] - 8s 194us/sample - loss: 0.6896 - sparse_categorical_accuracy: 0.7930\n",
      "Epoch 6/7\n",
      "41073/41073 [==============================] - 8s 194us/sample - loss: 0.6337 - sparse_categorical_accuracy: 0.8088\n",
      "Epoch 7/7\n",
      "41073/41073 [==============================] - 8s 191us/sample - loss: 0.5895 - sparse_categorical_accuracy: 0.8252\n",
      "\n",
      "Resultados con datos de testeo \n",
      "\n",
      "10269/10269 [==============================] - 2s 203us/sample - loss: 0.8092 - sparse_categorical_accuracy: 0.7560\n",
      "\n",
      "Data Test accuracy: 0.7559645771980286\n",
      "-Matriz de confusion\n",
      "[[ 802   36   69   23   11   53   37   67   23]\n",
      " [  16 1162   15   21    5   28   31   51   20]\n",
      " [  41   27  316   18   20   37   14   36   12]\n",
      " [  18   22   25  972   11   86   45   44   25]\n",
      " [  22   28   37   33  280   45   26   30   14]\n",
      " [  42   29   38  101   13 1588   37  100   27]\n",
      " [  28   39   13   33    5   31 1042   51   11]\n",
      " [  46   80   45   47    8  102   50  684   50]\n",
      " [  15   56   13   47    3   36   23   65  917]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.75800955, 0.7533353, 0.7598598, 0.75839907, 0.7559646]"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_len= len(vocabulary)+1\n",
    "num_classes = len(list_dimensions) #9\n",
    "ouput_dim_embedding = 100\n",
    "lstm_units = 200\n",
    "ouputs_dense = num_classes\n",
    "epoch = 7\n",
    "batch_size = 100\n",
    "optimizer = 'adam'\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "#Se agrega capa embedding que hace w2v\n",
    "model.add(tf.keras.layers.Embedding(input_dim=vocab_len, output_dim = ouput_dim_embedding, input_shape=(X_completo.shape[1],)))\n",
    "#model.add(tf.keras.layers.SpatialDropout1D(0.4))\n",
    "model.add(tf.keras.layers.LSTM(units=lstm_units, activation='sigmoid', dropout=0.7, recurrent_dropout=0.7))\n",
    "model.add(tf.keras.layers.Dense(units=ouputs_dense, activation='sigmoid'))\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "history = model.fit(X, y, epochs = epoch, batch_size=batch_size)\n",
    "print()\n",
    "print('Resultados con datos de testeo ')\n",
    "print()\n",
    "test_loss, test_acurracy = model.evaluate(test_x, test_y)\n",
    "print()\n",
    "print('Data Test accuracy: {}'.format(test_acurracy))\n",
    "predictions = model.predict_classes(test_x)\n",
    "resultados.append(test_acurracy)\n",
    "#Matriz de confusion\n",
    "confusion = tf.confusion_matrix(labels=test_y, predictions= predictions, dtype=tf.dtypes.int32, num_classes=num_classes)\n",
    "print('-Matriz de confusion')\n",
    "print(confusion.eval(session=tf.Session()))\n",
    "resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1422952,
     "status": "ok",
     "timestamp": 1576100583341,
     "user": {
      "displayName": "Ricardo Alvarez",
      "photoUrl": "",
      "userId": "07173409030660719160"
     },
     "user_tz": 180
    },
    "id": "A66UiERjNIkq",
    "outputId": "664c49b1-6d13-41f8-b306-b143ba632b8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 46207 samples\n",
      "Epoch 1/7\n",
      "46207/46207 [==============================] - 20s 427us/sample - loss: 1.7527 - sparse_categorical_accuracy: 0.3927\n",
      "Epoch 2/7\n",
      "46207/46207 [==============================] - 19s 415us/sample - loss: 1.0083 - sparse_categorical_accuracy: 0.6928\n",
      "Epoch 3/7\n",
      "46207/46207 [==============================] - 19s 412us/sample - loss: 0.8484 - sparse_categorical_accuracy: 0.7470\n",
      "Epoch 4/7\n",
      "46207/46207 [==============================] - 19s 406us/sample - loss: 0.7774 - sparse_categorical_accuracy: 0.7666\n",
      "Epoch 5/7\n",
      "46207/46207 [==============================] - 19s 402us/sample - loss: 0.7315 - sparse_categorical_accuracy: 0.7782\n",
      "Epoch 6/7\n",
      "46207/46207 [==============================] - 19s 409us/sample - loss: 0.6973 - sparse_categorical_accuracy: 0.7868\n",
      "Epoch 7/7\n",
      "46207/46207 [==============================] - 19s 403us/sample - loss: 0.6736 - sparse_categorical_accuracy: 0.7930\n",
      "\n",
      "1 -Resultados con datos de testeo \n",
      "\n",
      "5135/5135 [==============================] - 1s 267us/sample - loss: 0.8049 - sparse_categorical_accuracy: 0.7562\n",
      "\n",
      "1 -Data Test accuracy: 0.7561830282211304\n",
      "\n",
      "1 -Matriz de confusion\n",
      "[[469   6  11  27   5  40  10  18  11]\n",
      " [ 17 518   4  31   7  22  11  30  23]\n",
      " [ 32   2 150  19   9  26   6  15   4]\n",
      " [ 14   6   5 502   2  40  11   7  12]\n",
      " [ 20   6  14  30 142  25   5   7   5]\n",
      " [ 20   9   7  71   7 848   8  20   9]\n",
      " [ 43   8   4  31   4  26 492  20   3]\n",
      " [ 46  25  15  31   6  64  27 294  32]\n",
      " [ 15  14   6  36   3  22   8  17 468]]\n",
      "\n",
      "Train on 46207 samples\n",
      "Epoch 1/7\n",
      "46207/46207 [==============================] - 19s 414us/sample - loss: 1.7798 - sparse_categorical_accuracy: 0.3656\n",
      "Epoch 2/7\n",
      "46207/46207 [==============================] - 19s 421us/sample - loss: 1.0141 - sparse_categorical_accuracy: 0.6919\n",
      "Epoch 3/7\n",
      "46207/46207 [==============================] - 20s 425us/sample - loss: 0.8601 - sparse_categorical_accuracy: 0.7394\n",
      "Epoch 4/7\n",
      "46207/46207 [==============================] - 19s 416us/sample - loss: 0.7929 - sparse_categorical_accuracy: 0.7593\n",
      "Epoch 5/7\n",
      "46207/46207 [==============================] - 19s 418us/sample - loss: 0.7459 - sparse_categorical_accuracy: 0.7738\n",
      "Epoch 6/7\n",
      "46207/46207 [==============================] - 19s 417us/sample - loss: 0.7085 - sparse_categorical_accuracy: 0.7839\n",
      "Epoch 7/7\n",
      "46207/46207 [==============================] - 19s 418us/sample - loss: 0.6804 - sparse_categorical_accuracy: 0.7908\n",
      "\n",
      "2 -Resultados con datos de testeo \n",
      "\n",
      "5135/5135 [==============================] - 1s 275us/sample - loss: 0.8370 - sparse_categorical_accuracy: 0.7519\n",
      "\n",
      "2 -Data Test accuracy: 0.7518987059593201\n",
      "\n",
      "2 -Matriz de confusion\n",
      "[[413  20  63   5   9  12  24  10  12]\n",
      " [  9 617  21   3   4   7  18  14  13]\n",
      " [ 18  11 184   3   9  11   5  12   2]\n",
      " [ 14  18  40 455  10  31  23  15  23]\n",
      " [ 12  13  46   8 132  13  11   8   9]\n",
      " [ 25  30  60  42  14 709  27  29  22]\n",
      " [ 18  21  22   7   1   7 536  13   8]\n",
      " [ 26  57  50  11   5  22  29 346  27]\n",
      " [  2  25  20  10   4   8  13  10 469]]\n",
      "\n",
      "Train on 46208 samples\n",
      "Epoch 1/7\n",
      "46208/46208 [==============================] - 20s 423us/sample - loss: 1.7804 - sparse_categorical_accuracy: 0.3704\n",
      "Epoch 2/7\n",
      "46208/46208 [==============================] - 19s 410us/sample - loss: 1.0149 - sparse_categorical_accuracy: 0.6983\n",
      "Epoch 3/7\n",
      "46208/46208 [==============================] - 19s 406us/sample - loss: 0.8546 - sparse_categorical_accuracy: 0.7444\n",
      "Epoch 4/7\n",
      "46208/46208 [==============================] - 19s 408us/sample - loss: 0.7889 - sparse_categorical_accuracy: 0.7608\n",
      "Epoch 5/7\n",
      "46208/46208 [==============================] - 19s 405us/sample - loss: 0.7443 - sparse_categorical_accuracy: 0.7733\n",
      "Epoch 6/7\n",
      "46208/46208 [==============================] - 19s 401us/sample - loss: 0.7102 - sparse_categorical_accuracy: 0.7847\n",
      "Epoch 7/7\n",
      "46208/46208 [==============================] - 19s 412us/sample - loss: 0.6833 - sparse_categorical_accuracy: 0.7916\n",
      "\n",
      "3 -Resultados con datos de testeo \n",
      "\n",
      "5134/5134 [==============================] - 1s 289us/sample - loss: 0.8163 - sparse_categorical_accuracy: 0.7559\n",
      "\n",
      "3 -Data Test accuracy: 0.7559407949447632\n",
      "\n",
      "3 -Matriz de confusion\n",
      "[[370  12   7  13   4  31  62  42   8]\n",
      " [  6 571   0  10   3  10  26  24   7]\n",
      " [ 27   6 108  15   9  26  38  30   6]\n",
      " [  2  10   3 475   2  53  39  19   7]\n",
      " [ 13  10   2  15 155  32  31  17   4]\n",
      " [  8  25   2  49   5 845  41  53  15]\n",
      " [  7  11   0  17   4  13 575  17   5]\n",
      " [ 12  33   5  20   4  39  43 327  20]\n",
      " [  6  25   1  25   4  13  24  26 455]]\n",
      "\n",
      "Train on 46208 samples\n",
      "Epoch 1/7\n",
      "46208/46208 [==============================] - 20s 442us/sample - loss: 1.7831 - sparse_categorical_accuracy: 0.3634\n",
      "Epoch 2/7\n",
      "46208/46208 [==============================] - 19s 418us/sample - loss: 1.0392 - sparse_categorical_accuracy: 0.6880\n",
      "Epoch 3/7\n",
      "46208/46208 [==============================] - 19s 419us/sample - loss: 0.8572 - sparse_categorical_accuracy: 0.7441\n",
      "Epoch 4/7\n",
      "46208/46208 [==============================] - 19s 420us/sample - loss: 0.7767 - sparse_categorical_accuracy: 0.7657\n",
      "Epoch 5/7\n",
      "46208/46208 [==============================] - 19s 413us/sample - loss: 0.7289 - sparse_categorical_accuracy: 0.7786\n",
      "Epoch 6/7\n",
      "46208/46208 [==============================] - 19s 413us/sample - loss: 0.6971 - sparse_categorical_accuracy: 0.7875\n",
      "Epoch 7/7\n",
      "46208/46208 [==============================] - 19s 413us/sample - loss: 0.6700 - sparse_categorical_accuracy: 0.7948\n",
      "\n",
      "4 -Resultados con datos de testeo \n",
      "\n",
      "5134/5134 [==============================] - 1s 288us/sample - loss: 0.8344 - sparse_categorical_accuracy: 0.7450\n",
      "\n",
      "4 -Data Test accuracy: 0.7450330853462219\n",
      "\n",
      "4 -Matriz de confusion\n",
      "[[403   7  23  19   3  23  28  11   5]\n",
      " [ 25 530   5  15  10  26  27  20  17]\n",
      " [ 40   3 153   9   5  26  13   6   2]\n",
      " [ 19   6  11 474   8  57  34  11  13]\n",
      " [ 14   5  28  17 137  28  15   9   5]\n",
      " [ 35   6  17  35   8 840  26  34  13]\n",
      " [ 32   9   7  12   3  24 510  15   5]\n",
      " [ 32  22  19  28   5  71  43 330  33]\n",
      " [ 23  18   5  19   1  16  26  19 448]]\n",
      "\n",
      "Train on 46208 samples\n",
      "Epoch 1/7\n",
      "46208/46208 [==============================] - 20s 427us/sample - loss: 1.7494 - sparse_categorical_accuracy: 0.3881\n",
      "Epoch 2/7\n",
      "46208/46208 [==============================] - 19s 412us/sample - loss: 1.0136 - sparse_categorical_accuracy: 0.6873\n",
      "Epoch 3/7\n",
      "46208/46208 [==============================] - 19s 409us/sample - loss: 0.8516 - sparse_categorical_accuracy: 0.7444\n",
      "Epoch 4/7\n",
      "46208/46208 [==============================] - 19s 410us/sample - loss: 0.7735 - sparse_categorical_accuracy: 0.7667\n",
      "Epoch 5/7\n",
      "46208/46208 [==============================] - 19s 421us/sample - loss: 0.7269 - sparse_categorical_accuracy: 0.7787\n",
      "Epoch 6/7\n",
      "46208/46208 [==============================] - 20s 428us/sample - loss: 0.6954 - sparse_categorical_accuracy: 0.7880\n",
      "Epoch 7/7\n",
      "46208/46208 [==============================] - 20s 423us/sample - loss: 0.6692 - sparse_categorical_accuracy: 0.7955\n",
      "\n",
      "5 -Resultados con datos de testeo \n",
      "\n",
      "5134/5134 [==============================] - 1s 290us/sample - loss: 0.8590 - sparse_categorical_accuracy: 0.7384\n",
      "\n",
      "5 -Data Test accuracy: 0.7384105920791626\n",
      "\n",
      "5 -Matriz de confusion\n",
      "[[370   7  37  22  35  44  19  18  12]\n",
      " [  9 551   8  21  37  29   5  22  24]\n",
      " [ 20   0 162   8  29  21   2  16   4]\n",
      " [  6   9   9 516  32  54   7   8   9]\n",
      " [  7  10  17  21 178  21   4   6   5]\n",
      " [ 16   8  17  70  24 819   7  19  10]\n",
      " [ 17  14   7  32  23  29 449   9   9]\n",
      " [ 25  26  20  38  22  64  16 317  29]\n",
      " [  5  14   2  28  22  29   3  15 429]]\n",
      "\n",
      "Train on 46208 samples\n",
      "Epoch 1/7\n",
      "46208/46208 [==============================] - 20s 434us/sample - loss: 1.7672 - sparse_categorical_accuracy: 0.3787\n",
      "Epoch 2/7\n",
      "46208/46208 [==============================] - 19s 419us/sample - loss: 1.0343 - sparse_categorical_accuracy: 0.6866\n",
      "Epoch 3/7\n",
      "46208/46208 [==============================] - 19s 411us/sample - loss: 0.8539 - sparse_categorical_accuracy: 0.7445\n",
      "Epoch 4/7\n",
      "46208/46208 [==============================] - 19s 414us/sample - loss: 0.7778 - sparse_categorical_accuracy: 0.7656\n",
      "Epoch 5/7\n",
      "46208/46208 [==============================] - 19s 413us/sample - loss: 0.7290 - sparse_categorical_accuracy: 0.7786\n",
      "Epoch 6/7\n",
      "46208/46208 [==============================] - 19s 405us/sample - loss: 0.6951 - sparse_categorical_accuracy: 0.7891\n",
      "Epoch 7/7\n",
      "46208/46208 [==============================] - 19s 406us/sample - loss: 0.6700 - sparse_categorical_accuracy: 0.7955\n",
      "\n",
      "6 -Resultados con datos de testeo \n",
      "\n",
      "5134/5134 [==============================] - 2s 307us/sample - loss: 0.7918 - sparse_categorical_accuracy: 0.7624\n",
      "\n",
      "6 -Data Test accuracy: 0.7623685002326965\n",
      "\n",
      "6 -Matriz de confusion\n",
      "[[463  17  16   8   8  26  20  15  13]\n",
      " [ 13 571   5   7   4  10  13  15  14]\n",
      " [ 38  14 136  13  12  18   7  10   6]\n",
      " [ 18  18   2 499   5  41  28  16  17]\n",
      " [ 15  23  14  20 138  19   8   8  11]\n",
      " [ 35  23  14  56  11 817  22  33  17]\n",
      " [ 13  23   6  15   1  11 526   6   9]\n",
      " [ 41  45  17  26   3  58  26 311  25]\n",
      " [  8  21   2  24   2  14   9  19 453]]\n",
      "\n",
      "Train on 46208 samples\n",
      "Epoch 1/7\n",
      "46208/46208 [==============================] - 20s 425us/sample - loss: 1.7429 - sparse_categorical_accuracy: 0.3872\n",
      "Epoch 2/7\n",
      "46208/46208 [==============================] - 19s 407us/sample - loss: 0.9718 - sparse_categorical_accuracy: 0.7094\n",
      "Epoch 3/7\n",
      "46208/46208 [==============================] - 20s 425us/sample - loss: 0.8343 - sparse_categorical_accuracy: 0.7513\n",
      "Epoch 4/7\n",
      "46208/46208 [==============================] - 20s 430us/sample - loss: 0.7720 - sparse_categorical_accuracy: 0.7673\n",
      "Epoch 5/7\n",
      "46208/46208 [==============================] - 20s 427us/sample - loss: 0.7305 - sparse_categorical_accuracy: 0.7794\n",
      "Epoch 6/7\n",
      "46208/46208 [==============================] - 19s 417us/sample - loss: 0.7024 - sparse_categorical_accuracy: 0.7891\n",
      "Epoch 7/7\n",
      "46208/46208 [==============================] - 19s 421us/sample - loss: 0.6766 - sparse_categorical_accuracy: 0.7949\n",
      "\n",
      "7 -Resultados con datos de testeo \n",
      "\n",
      "5134/5134 [==============================] - 2s 318us/sample - loss: 0.9563 - sparse_categorical_accuracy: 0.7271\n",
      "\n",
      "7 -Data Test accuracy: 0.7271133661270142\n",
      "\n",
      "7 -Matriz de confusion\n",
      "[[397   7   8  37   5  16  77   7   9]\n",
      " [  4 518   0  33   1  11  57  13  17]\n",
      " [ 54   4  95  35   6  29  25  10  11]\n",
      " [  5   4   0 547   1  28  45   5   3]\n",
      " [ 16   6   3  63 123  19  19   6   7]\n",
      " [ 13   8   3 118   4 789  65  12  11]\n",
      " [  9   3   0  27   1   6 537   1   6]\n",
      " [ 23  27   3  49   6  60  81 272  34]\n",
      " [  8   4   4  43   0  17  42   7 455]]\n",
      "\n",
      "Train on 46208 samples\n",
      "Epoch 1/7\n",
      "46208/46208 [==============================] - 20s 432us/sample - loss: 1.7507 - sparse_categorical_accuracy: 0.3877\n",
      "Epoch 2/7\n",
      "46208/46208 [==============================] - 19s 418us/sample - loss: 0.9785 - sparse_categorical_accuracy: 0.7033\n",
      "Epoch 3/7\n",
      "46208/46208 [==============================] - 19s 416us/sample - loss: 0.8393 - sparse_categorical_accuracy: 0.7500\n",
      "Epoch 4/7\n",
      "46208/46208 [==============================] - 19s 409us/sample - loss: 0.7683 - sparse_categorical_accuracy: 0.7679\n",
      "Epoch 5/7\n",
      "46208/46208 [==============================] - 19s 413us/sample - loss: 0.7220 - sparse_categorical_accuracy: 0.7800\n",
      "Epoch 6/7\n",
      "46208/46208 [==============================] - 19s 411us/sample - loss: 0.6907 - sparse_categorical_accuracy: 0.7890\n",
      "Epoch 7/7\n",
      "46208/46208 [==============================] - 19s 408us/sample - loss: 0.6664 - sparse_categorical_accuracy: 0.7959\n",
      "\n",
      "8 -Resultados con datos de testeo \n",
      "\n",
      "5134/5134 [==============================] - 2s 313us/sample - loss: 0.8088 - sparse_categorical_accuracy: 0.7563\n",
      "\n",
      "8 -Data Test accuracy: 0.7563303709030151\n",
      "\n",
      "8 -Matriz de confusion\n",
      "[[397  31  20  10  25  39  17  22  15]\n",
      " [  7 630   3   5  11  11   7  21  11]\n",
      " [ 24  22 131   2  28  19  11  15   3]\n",
      " [ 15  20   4 418  20  61  24  31  17]\n",
      " [  5  14   5  16 170  15  10   8   6]\n",
      " [ 17  31  10  37  20 784  17  47  11]\n",
      " [ 12  26   4   4   9   8 489  20   6]\n",
      " [ 19  42   9  11   8  55  22 398  27]\n",
      " [  3  36   3  14  12  18  10  33 466]]\n",
      "\n",
      "Train on 46208 samples\n",
      "Epoch 1/7\n",
      "46208/46208 [==============================] - 20s 439us/sample - loss: 1.7432 - sparse_categorical_accuracy: 0.3922\n",
      "Epoch 2/7\n",
      "46208/46208 [==============================] - 20s 431us/sample - loss: 0.9543 - sparse_categorical_accuracy: 0.7145\n",
      "Epoch 3/7\n",
      "46208/46208 [==============================] - 19s 420us/sample - loss: 0.8151 - sparse_categorical_accuracy: 0.7548\n",
      "Epoch 4/7\n",
      "46208/46208 [==============================] - 20s 422us/sample - loss: 0.7548 - sparse_categorical_accuracy: 0.7717\n",
      "Epoch 5/7\n",
      "46208/46208 [==============================] - 19s 422us/sample - loss: 0.7183 - sparse_categorical_accuracy: 0.7814\n",
      "Epoch 6/7\n",
      "46208/46208 [==============================] - 19s 422us/sample - loss: 0.6900 - sparse_categorical_accuracy: 0.7903\n",
      "Epoch 7/7\n",
      "46208/46208 [==============================] - 19s 413us/sample - loss: 0.6684 - sparse_categorical_accuracy: 0.7964\n",
      "\n",
      "9 -Resultados con datos de testeo \n",
      "\n",
      "5134/5134 [==============================] - 2s 319us/sample - loss: 0.8199 - sparse_categorical_accuracy: 0.7513\n",
      "\n",
      "9 -Data Test accuracy: 0.7512660622596741\n",
      "\n",
      "9 -Matriz de confusion\n",
      "[[417   5  24  15   5  40  11  52   6]\n",
      " [  5 528   7   8   6  19  18  62   7]\n",
      " [ 45   5 141   7   6  31   4  29   3]\n",
      " [  8   4   6 474   7  52  14  43   8]\n",
      " [ 14  17  21  16 146  26   6  30   0]\n",
      " [ 12   5  13  36   8 819  11  53   9]\n",
      " [ 20  14   3  18   2  20 501  36   3]\n",
      " [ 21  23  10  12   3  55  21 411  15]\n",
      " [  8  22   5  15   4  27  11  70 420]]\n",
      "\n",
      "Train on 46208 samples\n",
      "Epoch 1/7\n",
      "46208/46208 [==============================] - 20s 431us/sample - loss: 1.7486 - sparse_categorical_accuracy: 0.4013\n",
      "Epoch 2/7\n",
      "46208/46208 [==============================] - 19s 409us/sample - loss: 0.9582 - sparse_categorical_accuracy: 0.7147\n",
      "Epoch 3/7\n",
      "46208/46208 [==============================] - 19s 411us/sample - loss: 0.8216 - sparse_categorical_accuracy: 0.7549\n",
      "Epoch 4/7\n",
      "46208/46208 [==============================] - 19s 408us/sample - loss: 0.7613 - sparse_categorical_accuracy: 0.7696\n",
      "Epoch 5/7\n",
      "46208/46208 [==============================] - 19s 405us/sample - loss: 0.7217 - sparse_categorical_accuracy: 0.7804\n",
      "Epoch 6/7\n",
      "46208/46208 [==============================] - 19s 407us/sample - loss: 0.6932 - sparse_categorical_accuracy: 0.7886\n",
      "Epoch 7/7\n",
      "46208/46208 [==============================] - 20s 425us/sample - loss: 0.6696 - sparse_categorical_accuracy: 0.7941\n",
      "\n",
      "10 -Resultados con datos de testeo \n",
      "\n",
      "5134/5134 [==============================] - 2s 338us/sample - loss: 0.8454 - sparse_categorical_accuracy: 0.7466\n",
      "\n",
      "10 -Data Test accuracy: 0.746591329574585\n",
      "\n",
      "10 -Matriz de confusion\n",
      "[[383  10  32  11   9  41  13  50  14]\n",
      " [  7 510   4   4   2  36  13  44  40]\n",
      " [ 20   4 127   6   6  33   4  28  12]\n",
      " [  7  11  11 493   9  95   8  36  36]\n",
      " [ 16   8  15   8 129  24   4  24  19]\n",
      " [  6  10  10  25   7 867  16  55  28]\n",
      " [ 11  16   6   7   2  43 431  25  15]\n",
      " [ 15  18   6  10   4  53   7 378  49]\n",
      " [  5   9   5   5   2  25   4  28 515]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab_len= len(vocabulary)+1\n",
    "num_classes = len(list_dimensions) #9\n",
    "ouput_dim_embedding = 60\n",
    "lstm_units = 100\n",
    "ouputs_dense = num_classes\n",
    "epoch = 7\n",
    "batch_size = 60\n",
    "optimizer = 'adam'\n",
    "\n",
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 2)\n",
    "resultados = []\n",
    "contador = 1\n",
    "for valores_entrenamiento, valores_testeo in kf.split(X_completo):\n",
    "    model = tf.keras.Sequential()\n",
    "    #Se agrega capa embedding que hace w2v\n",
    "    model.add(tf.keras.layers.Embedding(input_dim=vocab_len, output_dim = ouput_dim_embedding, input_shape=(X_completo.shape[1],)))\n",
    "    model.add(tf.keras.layers.LSTM(units=lstm_units, activation='sigmoid'))\n",
    "    model.add(tf.keras.layers.Dense(units=ouputs_dense, activation='sigmoid'))\n",
    "    model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "    history = model.fit(X_completo[valores_entrenamiento], y_completo[valores_entrenamiento], epochs = epoch, batch_size = batch_size)\n",
    "    print()\n",
    "    print(contador, '-Resultados con datos de testeo ')\n",
    "    print()\n",
    "    test_loss, test_acurracy = model.evaluate(X_completo[valores_testeo], y_completo[valores_testeo])\n",
    "    print()\n",
    "    print(contador, '-Data Test accuracy: {}'.format(test_acurracy))\n",
    "    resultados.append(test_acurracy)\n",
    "    print()\n",
    "    predictions = model.predict_classes(X_completo[valores_testeo])\n",
    "    #Matriz de confusion\n",
    "    confusion = tf.confusion_matrix(labels=y_completo[valores_testeo], predictions= predictions, dtype=tf.dtypes.int32, num_classes=num_classes)\n",
    "    print(contador, '-Matriz de confusion')\n",
    "    print(confusion.eval(session=tf.Session()))\n",
    "    print()\n",
    "    contador = contador + 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Soc_Int",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
