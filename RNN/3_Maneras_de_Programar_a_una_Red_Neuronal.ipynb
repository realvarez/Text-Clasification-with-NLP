{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vkHEHL9WLDeu"
   },
   "source": [
    "# 3 Maneras de Programar a una Red Neuronal - DOTCSV\n",
    "\n",
    "## Código inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 473
    },
    "colab_type": "code",
    "id": "40hacwtuJh5V",
    "outputId": "077040b8-5571-4c78-9bba-c546fd23b892"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Creamos nuestros datos artificiales, donde buscaremos clasificar \n",
    "# dos anillos concéntricos de datos. \n",
    "X, Y = make_circles(n_samples=500, factor=0.5, noise=0.05)\n",
    "\n",
    "# Resolución del mapa de predicción.\n",
    "res = 100 \n",
    "\n",
    "# Coordendadas del mapa de predicción.\n",
    "_x0 = np.linspace(-1.5, 1.5, res)\n",
    "_x1 = np.linspace(-1.5, 1.5, res)\n",
    "\n",
    "# Input con cada combo de coordenadas del mapa de predicción.\n",
    "_pX = np.array(np.meshgrid(_x0, _x1)).T.reshape(-1, 2)\n",
    "\n",
    "# Objeto vacio a 0.5 del mapa de predicción.\n",
    "_pY = np.zeros((res, res)) + 0.5\n",
    "\n",
    "# Visualización del mapa de predicción.\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.pcolormesh(_x0, _x1, _pY, cmap=\"coolwarm\", vmin=0, vmax=1)\n",
    "\n",
    "# Visualización de la nube de datos.\n",
    "plt.scatter(X[Y == 0,0], X[Y == 0,1], c=\"skyblue\")\n",
    "plt.scatter(X[Y == 1,0], X[Y == 1,1], c=\"salmon\")\n",
    "\n",
    "plt.tick_params(labelbottom=False, labelleft=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ec_TFbA5LAs-"
   },
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2017
    },
    "colab_type": "code",
    "id": "I4R-y55iJlAy",
    "outputId": "f7dbff5b-41f8-4056-96a4-e07f3610b5a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 / 1000 - Loss =  0.497612 - Acc = 0.5\n",
      "Step 25 / 1000 - Loss =  0.49620366 - Acc = 0.5\n",
      "Step 50 / 1000 - Loss =  0.49159634 - Acc = 0.5\n",
      "Step 75 / 1000 - Loss =  0.4630686 - Acc = 0.5\n",
      "Step 100 / 1000 - Loss =  0.43478718 - Acc = 0.528\n",
      "Step 125 / 1000 - Loss =  0.41380832 - Acc = 0.536\n",
      "Step 150 / 1000 - Loss =  0.38311782 - Acc = 0.578\n",
      "Step 175 / 1000 - Loss =  0.3477325 - Acc = 0.622\n",
      "Step 200 / 1000 - Loss =  0.3174123 - Acc = 0.66\n",
      "Step 225 / 1000 - Loss =  0.2875713 - Acc = 0.712\n",
      "Step 250 / 1000 - Loss =  0.25800508 - Acc = 0.748\n",
      "Step 275 / 1000 - Loss =  0.22925958 - Acc = 0.778\n",
      "Step 300 / 1000 - Loss =  0.17802241 - Acc = 0.818\n",
      "Step 325 / 1000 - Loss =  0.09153736 - Acc = 0.94\n",
      "Step 350 / 1000 - Loss =  0.061156966 - Acc = 0.996\n",
      "Step 375 / 1000 - Loss =  0.051044855 - Acc = 0.998\n",
      "Step 400 / 1000 - Loss =  0.045005105 - Acc = 0.998\n",
      "Step 425 / 1000 - Loss =  0.040735655 - Acc = 1.0\n",
      "Step 450 / 1000 - Loss =  0.03743945 - Acc = 1.0\n",
      "Step 475 / 1000 - Loss =  0.034789845 - Acc = 1.0\n",
      "Step 500 / 1000 - Loss =  0.032600205 - Acc = 1.0\n",
      "Step 525 / 1000 - Loss =  0.030759413 - Acc = 1.0\n",
      "Step 550 / 1000 - Loss =  0.029155845 - Acc = 1.0\n",
      "Step 575 / 1000 - Loss =  0.027747393 - Acc = 1.0\n",
      "Step 600 / 1000 - Loss =  0.026492277 - Acc = 1.0\n",
      "Step 625 / 1000 - Loss =  0.025365518 - Acc = 1.0\n",
      "Step 650 / 1000 - Loss =  0.024339076 - Acc = 1.0\n",
      "Step 675 / 1000 - Loss =  0.023397533 - Acc = 1.0\n",
      "Step 700 / 1000 - Loss =  0.02252903 - Acc = 1.0\n",
      "Step 725 / 1000 - Loss =  0.021724494 - Acc = 1.0\n",
      "Step 750 / 1000 - Loss =  0.020976113 - Acc = 1.0\n",
      "Step 775 / 1000 - Loss =  0.020277571 - Acc = 1.0\n",
      "Step 800 / 1000 - Loss =  0.019609682 - Acc = 1.0\n",
      "Step 825 / 1000 - Loss =  0.018993914 - Acc = 1.0\n",
      "Step 850 / 1000 - Loss =  0.018412774 - Acc = 1.0\n",
      "Step 875 / 1000 - Loss =  0.017864052 - Acc = 1.0\n",
      "Step 900 / 1000 - Loss =  0.017344143 - Acc = 1.0\n",
      "Step 925 / 1000 - Loss =  0.016848 - Acc = 1.0\n",
      "Step 950 / 1000 - Loss =  0.016377378 - Acc = 1.0\n",
      "Step 975 / 1000 - Loss =  0.015931351 - Acc = 1.0\n",
      "--- Generando animación ---\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Requested MovieWriter (ffmpeg) not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\matplotlib\\animation.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavail\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ffmpeg'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8bc704c134a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[0mani\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manimation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArtistAnimation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepeat_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m \u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mani\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_html5_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\matplotlib\\animation.py\u001b[0m in \u001b[0;36mto_html5_video\u001b[1;34m(self, embed_limit)\u001b[0m\n\u001b[0;32m   1324\u001b[0m                 \u001b[1;31m# We create a writer manually so that we can get the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m                 \u001b[1;31m# appropriate size for the tag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1326\u001b[1;33m                 \u001b[0mWriter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwriters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'animation.writer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1327\u001b[0m                 writer = Writer(codec='h264',\n\u001b[0;32m   1328\u001b[0m                                 \u001b[0mbitrate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'animation.bitrate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\matplotlib\\animation.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             raise RuntimeError(\n\u001b[1;32m--> 164\u001b[1;33m                 'Requested MovieWriter ({}) not available'.format(name))\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Requested MovieWriter (ffmpeg) not available"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from matplotlib import animation\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# Definimos los puntos de entrada de la red, para la matriz X e Y.\n",
    "iX = tf.placeholder('float', shape=[None, X.shape[1]])\n",
    "iY = tf.placeholder('float', shape=[None])\n",
    "\n",
    "lr = 0.01           # learning rate\n",
    "nn = [2, 16, 8, 1]  # número de neuronas por capa.\n",
    "\n",
    "# Capa 1\n",
    "W1 = tf.Variable(tf.random_normal([nn[0], nn[1]]), name='Weights_1')\n",
    "b1 = tf.Variable(tf.random_normal([nn[1]]), name='bias_1')\n",
    "\n",
    "l1 = tf.nn.relu(tf.add( tf.matmul(iX, W1), b1))\n",
    "\n",
    "# Capa 2\n",
    "W2 = tf.Variable(tf.random_normal([nn[1], nn[2]]), name='Weights_2')\n",
    "b2 = tf.Variable(tf.random_normal([nn[2]]), name='bias_2')\n",
    "\n",
    "l2 = tf.nn.relu(tf.add(tf.matmul(l1, W2), b2))\n",
    "\n",
    "# Capa 3\n",
    "W3 = tf.Variable(tf.random_normal([nn[2], nn[3]]), name='Weights_3')\n",
    "b3 = tf.Variable(tf.random_normal([nn[3]]), name='bias_3')\n",
    "\n",
    "# Vector de predicciones de Y.\n",
    "pY = tf.nn.sigmoid(tf.add(tf.matmul(l2, W3), b3))[:, 0]\n",
    "\n",
    "\n",
    "# Evaluación de las predicciones.\n",
    "loss = tf.losses.mean_squared_error(pY, iY)\n",
    "\n",
    "# Definimos al optimizador de la red, para que minimice el error.\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.05).minimize(loss)\n",
    "\n",
    "n_steps = 1000 # Número de ciclos de entrenamiento.\n",
    "\n",
    "iPY = [] # Aquí guardaremos la evolución de las predicción, para la animación.\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  \n",
    "  # Inicializamos todos los parámetros de la red, las matrices W y b.\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "  # Iteramos n pases de entrenamiento.\n",
    "  for step in range(n_steps):\n",
    "  \n",
    "    # Evaluamos al optimizador, a la función de coste y al tensor de salida pY. \n",
    "    # La evaluación del optimizer producirá el entrenamiento de la red.\n",
    "    _, _loss, _pY = sess.run([optimizer, loss, pY], feed_dict={ iX : X, iY : Y })\n",
    "    \n",
    "    # Cada 25 iteraciones, imprimimos métricas.\n",
    "    if step % 25 == 0: \n",
    "      \n",
    "      # Cálculo del accuracy.\n",
    "      acc = np.mean(np.round(_pY) == Y)\n",
    "      \n",
    "      # Impresión de métricas.\n",
    "      print('Step', step, '/', n_steps, '- Loss = ', _loss, '- Acc =', acc)\n",
    "      \n",
    "      # Obtenemos predicciones para cada punto de nuestro mapa de predicción _pX.\n",
    "      _pY = sess.run(pY, feed_dict={ iX : _pX }).reshape((res, res))\n",
    "\n",
    "      # Y lo guardamos para visualizar la animación.\n",
    "      iPY.append(_pY)\n",
    "      \n",
    "  \n",
    "# ----- CÓDIGO ANIMACIÓN ----- #\n",
    "\n",
    "ims = []\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "print(\"--- Generando animación ---\")\n",
    "\n",
    "for fr in range(len(iPY)):\n",
    "  \n",
    "  im = plt.pcolormesh(_x0, _x1, iPY[fr], cmap=\"coolwarm\", animated=True)\n",
    "\n",
    "  # Visualización de la nube de datos.\n",
    "  plt.scatter(X[Y == 0,0], X[Y == 0,1], c=\"skyblue\")\n",
    "  plt.scatter(X[Y == 1,0], X[Y == 1,1], c=\"salmon\")\n",
    "\n",
    "  # plt.title(\"Resultado Clasificación\")\n",
    "  plt.tick_params(labelbottom=False, labelleft=False)\n",
    "\n",
    "  ims.append([im])\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)\n",
    "\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0CbBPl9wK-kl"
   },
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3501
    },
    "colab_type": "code",
    "id": "OjW4ZNVeJoaC",
    "outputId": "3f4b7b2e-cbe9-4b17-b8f6-b548d8689874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "500/500 [==============================] - 0s 422us/sample - loss: 0.2439 - acc: 0.5100\n",
      "Epoch 2/100\n",
      "500/500 [==============================] - 0s 66us/sample - loss: 0.2430 - acc: 0.5400\n",
      "Epoch 3/100\n",
      "500/500 [==============================] - 0s 64us/sample - loss: 0.2421 - acc: 0.5600\n",
      "Epoch 4/100\n",
      "500/500 [==============================] - 0s 58us/sample - loss: 0.2412 - acc: 0.5860\n",
      "Epoch 5/100\n",
      "500/500 [==============================] - 0s 56us/sample - loss: 0.2403 - acc: 0.5620\n",
      "Epoch 6/100\n",
      "500/500 [==============================] - 0s 56us/sample - loss: 0.2393 - acc: 0.6300\n",
      "Epoch 7/100\n",
      "500/500 [==============================] - 0s 58us/sample - loss: 0.2384 - acc: 0.6080\n",
      "Epoch 8/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.2374 - acc: 0.6080\n",
      "Epoch 9/100\n",
      "500/500 [==============================] - 0s 58us/sample - loss: 0.2364 - acc: 0.6820\n",
      "Epoch 10/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.2353 - acc: 0.6480\n",
      "Epoch 11/100\n",
      "500/500 [==============================] - 0s 46us/sample - loss: 0.2343 - acc: 0.7180\n",
      "Epoch 12/100\n",
      "500/500 [==============================] - 0s 48us/sample - loss: 0.2332 - acc: 0.7200\n",
      "Epoch 13/100\n",
      "500/500 [==============================] - 0s 46us/sample - loss: 0.2320 - acc: 0.7080\n",
      "Epoch 14/100\n",
      "500/500 [==============================] - 0s 46us/sample - loss: 0.2309 - acc: 0.7280\n",
      "Epoch 15/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.2297 - acc: 0.7340\n",
      "Epoch 16/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.2284 - acc: 0.7640\n",
      "Epoch 17/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.2272 - acc: 0.7840\n",
      "Epoch 18/100\n",
      "500/500 [==============================] - 0s 49us/sample - loss: 0.2258 - acc: 0.8280\n",
      "Epoch 19/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.2244 - acc: 0.8440\n",
      "Epoch 20/100\n",
      "500/500 [==============================] - 0s 48us/sample - loss: 0.2230 - acc: 0.8340\n",
      "Epoch 21/100\n",
      "500/500 [==============================] - 0s 51us/sample - loss: 0.2213 - acc: 0.8520\n",
      "Epoch 22/100\n",
      "500/500 [==============================] - 0s 48us/sample - loss: 0.2195 - acc: 0.9180\n",
      "Epoch 23/100\n",
      "500/500 [==============================] - 0s 46us/sample - loss: 0.2177 - acc: 0.9100\n",
      "Epoch 24/100\n",
      "500/500 [==============================] - 0s 54us/sample - loss: 0.2159 - acc: 0.9260\n",
      "Epoch 25/100\n",
      "500/500 [==============================] - 0s 52us/sample - loss: 0.2141 - acc: 0.8980\n",
      "Epoch 26/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.2122 - acc: 0.9360\n",
      "Epoch 27/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.2102 - acc: 0.9400\n",
      "Epoch 28/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.2081 - acc: 0.9600\n",
      "Epoch 29/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.2059 - acc: 0.9620\n",
      "Epoch 30/100\n",
      "500/500 [==============================] - 0s 48us/sample - loss: 0.2037 - acc: 0.9660\n",
      "Epoch 31/100\n",
      "500/500 [==============================] - 0s 54us/sample - loss: 0.2013 - acc: 0.9680\n",
      "Epoch 32/100\n",
      "500/500 [==============================] - 0s 54us/sample - loss: 0.1986 - acc: 0.9760\n",
      "Epoch 33/100\n",
      "500/500 [==============================] - 0s 54us/sample - loss: 0.1959 - acc: 0.9900\n",
      "Epoch 34/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.1931 - acc: 0.9840\n",
      "Epoch 35/100\n",
      "500/500 [==============================] - 0s 56us/sample - loss: 0.1900 - acc: 0.9900\n",
      "Epoch 36/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.1869 - acc: 0.9920\n",
      "Epoch 37/100\n",
      "500/500 [==============================] - 0s 48us/sample - loss: 0.1834 - acc: 0.9880\n",
      "Epoch 38/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.1790 - acc: 0.9940\n",
      "Epoch 39/100\n",
      "500/500 [==============================] - 0s 52us/sample - loss: 0.1736 - acc: 1.0000\n",
      "Epoch 40/100\n",
      "500/500 [==============================] - 0s 48us/sample - loss: 0.1677 - acc: 0.9980\n",
      "Epoch 41/100\n",
      "500/500 [==============================] - 0s 46us/sample - loss: 0.1620 - acc: 1.0000\n",
      "Epoch 42/100\n",
      "500/500 [==============================] - 0s 46us/sample - loss: 0.1566 - acc: 1.0000\n",
      "Epoch 43/100\n",
      "500/500 [==============================] - 0s 52us/sample - loss: 0.1516 - acc: 1.0000\n",
      "Epoch 44/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.1469 - acc: 1.0000\n",
      "Epoch 45/100\n",
      "500/500 [==============================] - 0s 44us/sample - loss: 0.1425 - acc: 1.0000\n",
      "Epoch 46/100\n",
      "500/500 [==============================] - 0s 46us/sample - loss: 0.1381 - acc: 0.9980\n",
      "Epoch 47/100\n",
      "500/500 [==============================] - 0s 48us/sample - loss: 0.1337 - acc: 1.0000\n",
      "Epoch 48/100\n",
      "500/500 [==============================] - 0s 48us/sample - loss: 0.1292 - acc: 1.0000\n",
      "Epoch 49/100\n",
      "500/500 [==============================] - 0s 46us/sample - loss: 0.1251 - acc: 1.0000\n",
      "Epoch 50/100\n",
      "500/500 [==============================] - 0s 46us/sample - loss: 0.1208 - acc: 1.0000\n",
      "Epoch 51/100\n",
      "500/500 [==============================] - 0s 52us/sample - loss: 0.1162 - acc: 1.0000\n",
      "Epoch 52/100\n",
      "500/500 [==============================] - 0s 44us/sample - loss: 0.1120 - acc: 1.0000\n",
      "Epoch 53/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.1077 - acc: 1.0000\n",
      "Epoch 54/100\n",
      "500/500 [==============================] - 0s 48us/sample - loss: 0.1035 - acc: 1.0000\n",
      "Epoch 55/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.0994 - acc: 1.0000\n",
      "Epoch 56/100\n",
      "500/500 [==============================] - 0s 44us/sample - loss: 0.0953 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.0914 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.0874 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.0838 - acc: 1.0000\n",
      "Epoch 60/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.0800 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "500/500 [==============================] - 0s 52us/sample - loss: 0.0766 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "500/500 [==============================] - 0s 48us/sample - loss: 0.0733 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.0700 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "500/500 [==============================] - 0s 44us/sample - loss: 0.0668 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.0639 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "500/500 [==============================] - 0s 54us/sample - loss: 0.0610 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "500/500 [==============================] - 0s 52us/sample - loss: 0.0583 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.0557 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "500/500 [==============================] - 0s 49us/sample - loss: 0.0533 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "500/500 [==============================] - 0s 42us/sample - loss: 0.0509 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "500/500 [==============================] - 0s 46us/sample - loss: 0.0486 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.0466 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "500/500 [==============================] - 0s 48us/sample - loss: 0.0446 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "500/500 [==============================] - 0s 52us/sample - loss: 0.0426 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "500/500 [==============================] - 0s 54us/sample - loss: 0.0409 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "500/500 [==============================] - 0s 48us/sample - loss: 0.0391 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "500/500 [==============================] - 0s 52us/sample - loss: 0.0375 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "500/500 [==============================] - 0s 52us/sample - loss: 0.0360 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "500/500 [==============================] - 0s 48us/sample - loss: 0.0345 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "500/500 [==============================] - 0s 48us/sample - loss: 0.0332 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.0318 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.0306 - acc: 1.0000\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 50us/sample - loss: 0.0294 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "500/500 [==============================] - 0s 48us/sample - loss: 0.0283 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "500/500 [==============================] - 0s 49us/sample - loss: 0.0273 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "500/500 [==============================] - 0s 48us/sample - loss: 0.0263 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "500/500 [==============================] - 0s 52us/sample - loss: 0.0253 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "500/500 [==============================] - 0s 52us/sample - loss: 0.0245 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "500/500 [==============================] - 0s 46us/sample - loss: 0.0236 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "500/500 [==============================] - 0s 48us/sample - loss: 0.0228 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "500/500 [==============================] - 0s 46us/sample - loss: 0.0220 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "500/500 [==============================] - 0s 46us/sample - loss: 0.0213 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.0206 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "500/500 [==============================] - 0s 48us/sample - loss: 0.0199 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.0193 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.0187 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "500/500 [==============================] - 0s 44us/sample - loss: 0.0181 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "500/500 [==============================] - 0s 44us/sample - loss: 0.0176 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "500/500 [==============================] - 0s 46us/sample - loss: 0.0171 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "500/500 [==============================] - 0s 50us/sample - loss: 0.0166 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ac7ee59668>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as kr\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "\n",
    "lr = 0.01           # learning rate\n",
    "nn = [2, 16, 8, 1]  # número de neuronas por capa.\n",
    "\n",
    "\n",
    "# Creamos el objeto que contendrá a nuestra red neuronal, como\n",
    "# secuencia de capas.\n",
    "model = kr.Sequential()\n",
    "\n",
    "# Añadimos la capa 1\n",
    "l1 = model.add(kr.layers.Dense(nn[1], activation='relu'))\n",
    "\n",
    "# Añadimos la capa 2\n",
    "l2 = model.add(kr.layers.Dense(nn[2], activation='relu'))\n",
    "\n",
    "# Añadimos la capa 3\n",
    "l3 = model.add(kr.layers.Dense(nn[3], activation='sigmoid'))\n",
    "\n",
    "# Compilamos el modelo, definiendo la función de coste y el optimizador.\n",
    "model.compile(loss='mse', optimizer=kr.optimizers.SGD(lr=0.05), metrics=['acc'])\n",
    "\n",
    "# Y entrenamos al modelo. Los callbacks \n",
    "model.fit(X, Y, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K9VpEDTuK4zo"
   },
   "source": [
    "## Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3660
    },
    "colab_type": "code",
    "id": "6bm8XuL_Jq1S",
    "outputId": "ed833c89-b9ce-44c9-c230-1363bbf6ebb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.59889154\n",
      "Iteration 2, loss = 0.28390780\n",
      "Iteration 3, loss = 0.13625991\n",
      "Iteration 4, loss = 0.12699817\n",
      "Iteration 5, loss = 0.13302587\n",
      "Iteration 6, loss = 0.13013463\n",
      "Iteration 7, loss = 0.12638290\n",
      "Iteration 8, loss = 0.12509905\n",
      "Iteration 9, loss = 0.12504352\n",
      "Iteration 10, loss = 0.12513065\n",
      "Iteration 11, loss = 0.12512404\n",
      "Iteration 12, loss = 0.12503678\n",
      "Iteration 13, loss = 0.12503598\n",
      "Iteration 14, loss = 0.12503416\n",
      "Iteration 15, loss = 0.12505372\n",
      "Iteration 16, loss = 0.12503989\n",
      "Iteration 17, loss = 0.12509609\n",
      "Iteration 18, loss = 0.12504540\n",
      "Iteration 19, loss = 0.12512371\n",
      "Iteration 20, loss = 0.12513884\n",
      "Iteration 21, loss = 0.12507660\n",
      "Iteration 22, loss = 0.12523249\n",
      "Iteration 23, loss = 0.12513334\n",
      "Iteration 24, loss = 0.12503470\n",
      "Iteration 25, loss = 0.12512377\n",
      "Iteration 26, loss = 0.12505966\n",
      "Iteration 27, loss = 0.12515226\n",
      "Iteration 28, loss = 0.12511623\n",
      "Iteration 29, loss = 0.12505148\n",
      "Iteration 30, loss = 0.12514221\n",
      "Iteration 31, loss = 0.12506917\n",
      "Iteration 32, loss = 0.12502091\n",
      "Iteration 33, loss = 0.12509061\n",
      "Iteration 34, loss = 0.12504607\n",
      "Iteration 35, loss = 0.12505442\n",
      "Iteration 36, loss = 0.12518405\n",
      "Iteration 37, loss = 0.12506519\n",
      "Iteration 38, loss = 0.12512032\n",
      "Iteration 39, loss = 0.12506745\n",
      "Iteration 40, loss = 0.12505002\n",
      "Iteration 41, loss = 0.12503480\n",
      "Iteration 42, loss = 0.12508919\n",
      "Iteration 43, loss = 0.12506805\n",
      "Iteration 44, loss = 0.12505045\n",
      "Iteration 45, loss = 0.12505736\n",
      "Iteration 46, loss = 0.12519812\n",
      "Iteration 47, loss = 0.12509580\n",
      "Iteration 48, loss = 0.12505906\n",
      "Iteration 49, loss = 0.12505134\n",
      "Iteration 50, loss = 0.12507316\n",
      "Iteration 51, loss = 0.12504131\n",
      "Iteration 52, loss = 0.12504574\n",
      "Iteration 53, loss = 0.12504628\n",
      "Iteration 54, loss = 0.12506394\n",
      "Iteration 55, loss = 0.12504938\n",
      "Iteration 56, loss = 0.12509352\n",
      "Iteration 57, loss = 0.12502892\n",
      "Iteration 58, loss = 0.12508003\n",
      "Iteration 59, loss = 0.12503480\n",
      "Iteration 60, loss = 0.12504683\n",
      "Iteration 61, loss = 0.12507417\n",
      "Iteration 62, loss = 0.12506389\n",
      "Iteration 63, loss = 0.12501567\n",
      "Iteration 64, loss = 0.12508689\n",
      "Iteration 65, loss = 0.12505228\n",
      "Iteration 66, loss = 0.12506726\n",
      "Iteration 67, loss = 0.12505032\n",
      "Iteration 68, loss = 0.12508135\n",
      "Iteration 69, loss = 0.12503856\n",
      "Iteration 70, loss = 0.12504467\n",
      "Iteration 71, loss = 0.12504134\n",
      "Iteration 72, loss = 0.12506644\n",
      "Iteration 73, loss = 0.12512409\n",
      "Iteration 74, loss = 0.12504038\n",
      "Iteration 75, loss = 0.12505969\n",
      "Iteration 76, loss = 0.12503630\n",
      "Iteration 77, loss = 0.12504900\n",
      "Iteration 78, loss = 0.12506801\n",
      "Iteration 79, loss = 0.12512390\n",
      "Iteration 80, loss = 0.12514959\n",
      "Iteration 81, loss = 0.12513313\n",
      "Iteration 82, loss = 0.12504551\n",
      "Iteration 83, loss = 0.12505002\n",
      "Iteration 84, loss = 0.12504739\n",
      "Iteration 85, loss = 0.12506514\n",
      "Iteration 86, loss = 0.12504387\n",
      "Iteration 87, loss = 0.12509061\n",
      "Iteration 88, loss = 0.12505593\n",
      "Iteration 89, loss = 0.12509193\n",
      "Iteration 90, loss = 0.12509812\n",
      "Iteration 91, loss = 0.12505373\n",
      "Iteration 92, loss = 0.12505587\n",
      "Iteration 93, loss = 0.12501997\n",
      "Iteration 94, loss = 0.12507495\n",
      "Iteration 95, loss = 0.12505585\n",
      "Iteration 96, loss = 0.12508109\n",
      "Iteration 97, loss = 0.12508088\n",
      "Iteration 98, loss = 0.12506809\n",
      "Iteration 99, loss = 0.12506141\n",
      "Iteration 100, loss = 0.12517539\n",
      "Iteration 101, loss = 0.12504468\n",
      "Iteration 102, loss = 0.12503177\n",
      "Iteration 103, loss = 0.12509449\n",
      "Iteration 104, loss = 0.12510286\n",
      "Iteration 105, loss = 0.12506552\n",
      "Iteration 106, loss = 0.12507942\n",
      "Iteration 107, loss = 0.12503547\n",
      "Iteration 108, loss = 0.12505181\n",
      "Iteration 109, loss = 0.12503742\n",
      "Iteration 110, loss = 0.12505949\n",
      "Iteration 111, loss = 0.12504721\n",
      "Iteration 112, loss = 0.12502278\n",
      "Iteration 113, loss = 0.12503558\n",
      "Iteration 114, loss = 0.12503915\n",
      "Iteration 115, loss = 0.12502146\n",
      "Iteration 116, loss = 0.12503031\n",
      "Iteration 117, loss = 0.12503867\n",
      "Iteration 118, loss = 0.12502425\n",
      "Iteration 119, loss = 0.12505088\n",
      "Iteration 120, loss = 0.12514691\n",
      "Iteration 121, loss = 0.12504342\n",
      "Iteration 122, loss = 0.12505192\n",
      "Iteration 123, loss = 0.12508736\n",
      "Iteration 124, loss = 0.12505450\n",
      "Iteration 125, loss = 0.12505528\n",
      "Iteration 126, loss = 0.12508145\n",
      "Iteration 127, loss = 0.12507901\n",
      "Iteration 128, loss = 0.12505255\n",
      "Iteration 129, loss = 0.12504679\n",
      "Iteration 130, loss = 0.12504625\n",
      "Iteration 131, loss = 0.12507378\n",
      "Iteration 132, loss = 0.12507341\n",
      "Iteration 133, loss = 0.12502899\n",
      "Iteration 134, loss = 0.12502678\n",
      "Iteration 135, loss = 0.12504485\n",
      "Iteration 136, loss = 0.12507050\n",
      "Iteration 137, loss = 0.12503001\n",
      "Iteration 138, loss = 0.12509928\n",
      "Iteration 139, loss = 0.12513856\n",
      "Iteration 140, loss = 0.12506363\n",
      "Iteration 141, loss = 0.12503559\n",
      "Iteration 142, loss = 0.12506386\n",
      "Iteration 143, loss = 0.12508913\n",
      "Iteration 144, loss = 0.12502212\n",
      "Iteration 145, loss = 0.12507393\n",
      "Iteration 146, loss = 0.12515667\n",
      "Iteration 147, loss = 0.12502039\n",
      "Iteration 148, loss = 0.12503953\n",
      "Iteration 149, loss = 0.12512365\n",
      "Iteration 150, loss = 0.12503392\n",
      "Iteration 151, loss = 0.12504556\n",
      "Iteration 152, loss = 0.12506290\n",
      "Iteration 153, loss = 0.12505721\n",
      "Iteration 154, loss = 0.12506206\n",
      "Iteration 155, loss = 0.12504379\n",
      "Iteration 156, loss = 0.12510232\n",
      "Iteration 157, loss = 0.12505261\n",
      "Iteration 158, loss = 0.12502512\n",
      "Iteration 159, loss = 0.12505147\n",
      "Iteration 160, loss = 0.12503392\n",
      "Iteration 161, loss = 0.12506109\n",
      "Iteration 162, loss = 0.12507268\n",
      "Iteration 163, loss = 0.12509816\n",
      "Iteration 164, loss = 0.12504994\n",
      "Iteration 165, loss = 0.12504992\n",
      "Iteration 166, loss = 0.12509429\n",
      "Iteration 167, loss = 0.12517466\n",
      "Iteration 168, loss = 0.12503501\n",
      "Iteration 169, loss = 0.12508465\n",
      "Iteration 170, loss = 0.12504204\n",
      "Iteration 171, loss = 0.12509010\n",
      "Iteration 172, loss = 0.12503561\n",
      "Iteration 173, loss = 0.12518012\n",
      "Iteration 174, loss = 0.12504951\n",
      "Iteration 175, loss = 0.12506561\n",
      "Iteration 176, loss = 0.12504622\n",
      "Iteration 177, loss = 0.12506043\n",
      "Iteration 178, loss = 0.12508165\n",
      "Iteration 179, loss = 0.12504031\n",
      "Iteration 180, loss = 0.12503749\n",
      "Iteration 181, loss = 0.12504625\n",
      "Iteration 182, loss = 0.12503085\n",
      "Iteration 183, loss = 0.12508011\n",
      "Iteration 184, loss = 0.12506141\n",
      "Iteration 185, loss = 0.12504582\n",
      "Iteration 186, loss = 0.12502233\n",
      "Iteration 187, loss = 0.12503632\n",
      "Iteration 188, loss = 0.12510198\n",
      "Iteration 189, loss = 0.12506412\n",
      "Iteration 190, loss = 0.12502893\n",
      "Iteration 191, loss = 0.12502855\n",
      "Iteration 192, loss = 0.12502180\n",
      "Iteration 193, loss = 0.12507466\n",
      "Iteration 194, loss = 0.12504917\n",
      "Iteration 195, loss = 0.12509204\n",
      "Iteration 196, loss = 0.12505605\n",
      "Iteration 197, loss = 0.12504798\n",
      "Iteration 198, loss = 0.12507281\n",
      "Iteration 199, loss = 0.12504589\n",
      "Iteration 200, loss = 0.12506995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ralvarez\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size=64, beta_1=0.9,\n",
       "             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "             hidden_layer_sizes=(16, 8, 1), learning_rate='constant',\n",
       "             learning_rate_init=0.01, max_iter=200, momentum=0.9,\n",
       "             n_iter_no_change=1000, nesterovs_momentum=True, power_t=0.5,\n",
       "             random_state=None, shuffle=True, solver='sgd', tol=0.0001,\n",
       "             validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "import sklearn.neural_network\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "\n",
    "lr = 0.01           # learning rate\n",
    "nn = [2, 16, 8, 1]  # número de neuronas por capa.\n",
    "\n",
    "# Creamos el objeto del modelo de red neuronal multicapa.\n",
    "clf = sk.neural_network.MLPRegressor(solver='sgd', \n",
    "                                     learning_rate_init=lr, \n",
    "                                     hidden_layer_sizes=tuple(nn[1:]),\n",
    "                                     verbose=True,\n",
    "                                     n_iter_no_change=1000,\n",
    "                                     batch_size = 64)\n",
    "\n",
    "\n",
    "# Y lo entrenamos con nuestro datos.\n",
    "clf.fit(X, Y)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "3 Maneras de Programar a una Red Neuronal.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
