{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ricardo\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\Ricardo\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import num2words\n",
    "import boto3\n",
    "import pandas as uwu\n",
    "\n",
    "from num2words import num2words\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(list_tokens):\n",
    "    stop = stopwords.words('spanish')\n",
    "    for token in list_tokens:\n",
    "        if token.lower() in stop:\n",
    "            list_tokens.pop(tokens.index(token))\n",
    "    return tokens\n",
    "\n",
    "def remove_void_elements(tokens):\n",
    "    for token in tokens:\n",
    "        if token == '' or token == \" \" or token == \"  \":\n",
    "            tokens.pop(tokens.index(token))\n",
    "    return tokens\n",
    "\n",
    "def normalize_text(tokens):\n",
    "    for token in tokens:\n",
    "        pos = tokens.index(token)\n",
    "        if token.isdigit() or token.isalpha() == False:\n",
    "            tokens.pop(pos)\n",
    "        else:\n",
    "            token = re.sub(r'[^\\w\\s]', ' ', token)\n",
    "            if token != ' ' or token != '':\n",
    "                tokens[pos] = unicodedata.normalize('NFKD', token.lower()).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            else:\n",
    "                tokens.pop(pos)\n",
    "    return tokens\n",
    "\n",
    "def create_lemma_dict():\n",
    "    lemmaDiccionario = {}\n",
    "    with open('Herramientas/lemma.txt', 'rb') as fichero:\n",
    "        datos = (fichero.read().decode('utf8').replace(u'\\r', u'').split(u'\\n'))\n",
    "        datos = ([avance.split(u'\\t') for avance in datos])\n",
    "    for avance in datos:\n",
    "        if len(avance) >1:\n",
    "            lemmaDiccionario[avance[1]] = avance[0]\n",
    "            \n",
    "    return lemmaDiccionario\n",
    "        \n",
    "def lemmatize(word):\n",
    "    return lemmaDiccionario.get(word, word + u'')\n",
    "   \n",
    "def lemmatize_words(words):\n",
    "    lemmaDiccionario = create_lemma_dict()    \n",
    "    new_words = []\n",
    "    for palabra in words:\n",
    "        new_word = lemmatize(palabra)\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def wordsToNumbers(tokens, vocabulary):\n",
    "    number_array = []\n",
    "    for i in tokens:\n",
    "        number_array.append(vocabulary.index(i)+1)\n",
    "    return np.asarray(number_array)\n",
    "\n",
    "def word_to_vec(array_text, model):\n",
    "    model = Word2Vec.load('modelWord2vec.bin')\n",
    "    array_vectors = []\n",
    "    for word in array_text:\n",
    "        array_vectors.append(model.wv['word'])\n",
    "    return array_vectors\n",
    "\n",
    "def processText(Text):\n",
    "    list_text = nltk.word_tokenize(Text)\n",
    "    list_text = remove_stop_words(list_text)\n",
    "    list_text = remove_void_elements(list_text)\n",
    "    list_text = normalize_text(list_text)\n",
    "    list_text = lemmatize_words(list_text)    \n",
    "    if len(list_text)>10:\n",
    "        return list_text[:10]\n",
    "    else:\n",
    "        return list_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def lambdaFunction(text, area):\n",
    "    if (not isinstance(text, str) or len(text) < 5):\n",
    "        return 0\n",
    "    if (area not in ['Gobierno Corporativo', 'Medio Ambiente', 'Social Externo', 'Social Interno']):\n",
    "        return 0\n",
    "    text = processText(text)\n",
    "#     s3 = boto3.resource('s3')\n",
    "    vocab = uwu.readcsv('s3://titleproyectbucket2019/vocabulary.csv')\n",
    "    \n",
    "    if(area = 'Gobierno Corporativo'):\n",
    "        \n",
    "    else if( area = 'Medio Ambiente'):\n",
    "    else if( area = 'Social Externo'):\n",
    "    else if( area = 'Social Interno'):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
