{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ricardo\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\Ricardo\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import pandas as pd\n",
    "from unicodedata import normalize\n",
    "import pandas as pd\n",
    "import nltk.data\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import num2words\n",
    "from num2words import num2words\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import goslate\n",
    "import time\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = \"\"\n",
    "lemmaDiccionario = \"\"\n",
    "\n",
    "model = Word2Vec.load('modelWord2vec.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):\n",
    "    if stop == \"\":\n",
    "        stop = stopwords.words('spanish')\n",
    "    for token in tokens:\n",
    "        if token.lower() in stop:\n",
    "            tokens.pop(tokens.index(token))\n",
    "    return tokens    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_void_elements(tokens):\n",
    "    for token in tokens:\n",
    "        if token == '' or token == \" \" or token == \"  \":\n",
    "            tokens.pop(tokens.index(token))\n",
    "    return tokens\n",
    "\n",
    "def normalize_text(tokens):\n",
    "    for token in tokens:\n",
    "        pos = tokens.index(token)\n",
    "        if token.isdigit():\n",
    "            tokens[pos] = num2words(token, lang='es')\n",
    "        else:\n",
    "            token = re.sub(r'[^\\w\\s]', ' ', token)\n",
    "            if token != ' ' or token != '':\n",
    "                tokens[pos] = unicodedata.normalize('NFKD', token.lower()).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            else:\n",
    "                tokens.pop(pos)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lemma_dict():\n",
    "    lemmaDiccionario = {}\n",
    "    with open('Herramientas/lemma.txt', 'rb') as fichero:\n",
    "        datos = (fichero.read().decode('utf8').replace(u'\\r', u'').split(u'\\n'))\n",
    "        datos = ([avance.split(u'\\t') for avance in datos])\n",
    "    for avance in datos:\n",
    "        if len(avance) >1:\n",
    "            lemmaDiccionario[avance[1]] = avance[0]\n",
    "            \n",
    "    return lemmaDiccionario\n",
    "        \n",
    "def lemmatize(word):\n",
    "    return lemmaDiccionario.get(word, word + u'')\n",
    "   \n",
    "def lemmatize_words(words):\n",
    "    if lemmaDiccionario == \"\":\n",
    "        lemmaDiccionario = create_lemma_dict()\n",
    "        \n",
    "    new_words = []\n",
    "    for palabra in words:\n",
    "        new_word = lemmatize(palabra)\n",
    "        new_words.append(new_word)\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesing_text_sentence(sentence):\n",
    "    sentence = re.sub(r'[^\\w\\s]', ' ', sentence)\n",
    "    sentence = nltk.word_tokenice(sentence)\n",
    "    sentence = remove_stop_words(sentence)\n",
    "    sentence = remove_void_elements(sentence)\n",
    "    sentence = lemmatize_words(sentence)\n",
    "    if len(sentence)>10:\n",
    "        return sentence[:10]\n",
    "    else:\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_vec(array_text, model):\n",
    "    array_vectors = []\n",
    "    for word in array_text:\n",
    "        array_vectors.append(model.wv['word'])\n",
    "    return array_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_sentences_to_vector(sentences, model):\n",
    "    divided_sentence = nltk.sent_tokenize(sentences)\n",
    "    for _sentence in divided_sentence:\n",
    "        _sentence = procesing_text_sentence(_sentence)\n",
    "        divided_sentence[divided_sentence.index(_sentence)] = word_to_vec(_sentence, model)\n",
    "    return divided_sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
